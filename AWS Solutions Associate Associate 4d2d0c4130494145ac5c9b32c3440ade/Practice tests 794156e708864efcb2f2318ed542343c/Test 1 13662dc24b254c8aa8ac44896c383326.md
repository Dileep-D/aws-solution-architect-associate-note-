# Test 1

On an EBS-backed instance, the **default action is for the root EBS volume to be deleted** when the instance is terminated

DynamoDB Accelerator (DAX) feature is primarily used to significantly improve the in-memory read performance of your database

The main use case for RAID 1 is to provide mirroring, redundancy, and fault-tolerance. RAID 0 is a more suitable option for providing faster read and write operations, compared with RAID 1.

AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users).

AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).

Auto Scaling Group health check type: EC2 and ELB. When EC2 instance behind an ELB fails a health check:

- EC2 type: ELB stops sending traffic to EC2 instances
- ELB type: EC2 instance is replaced automatically by ELB

![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-48-03-503799cd694a657201456b3add758b53.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-48-03-503799cd694a657201456b3add758b53.png)

**You are working as a Solutions Architect for a major telecommunications company where you are assigned to improve the security of your database tier by tightly managing the data flow of your Amazon Redshift cluster. One of the requirements is to use VPC flow logs to monitor all the COPY and UNLOAD traffic of your Redshift cluster that moves in and out of your VPC. 

Which of the following is the most suitable solution to implement in this scenario?**

- ​Enable Audit Logging in your Amazon Redshift cluster.
- ​Use the Amazon Redshift Spectrum feature.
- ​Enable Enhanced VPC routing on your Amazon Redshift cluster
- ​Create a new flow log that tracks the traffic of your Amazon Redshift cluster.
- Answer

    When you use Amazon Redshift Enhanced VPC Routing, Amazon Redshift forces all COPY and UNLOAD traffic between your cluster and your data repositories through your Amazon VPC. By using Enhanced VPC Routing, you can use standard VPC features, such as VPC security groups, network access control lists (ACLs), VPC endpoints, VPC endpoint policies, internet gateways, and Domain Name System (DNS) servers. Hence, **enabling Enhanced VPC routing on your Amazon Redshift cluster** is the correct answer.

    You use these features to tightly manage the flow of data between your Amazon Redshift cluster and other resources. When you use Enhanced VPC Routing to route traffic through your VPC, you can also use VPC flow logs to monitor COPY and UNLOAD traffic. If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network.

    ![https://docs.aws.amazon.com/redshift/latest/mgmt/images/enhanced-routing-create.png](https://docs.aws.amazon.com/redshift/latest/mgmt/images/enhanced-routing-create.png)

    **Enabling Audit Logging in your Amazon Redshift cluster** is incorrect because the Audit Logging feature is primarily used to get the information about the connection, queries, and user activities in your Redshift cluster.

    **Using the Amazon Redshift Spectrum feature** is incorrect because this is primarily used to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required.

    - Quite similar is Athena

    [S3, CloudFront, Snowball, Athena vs Macie](../S3%20CloudFront%20Snowball%20Athena%20vs%20Macie%20f715df5f96a1408e991469501b27a745.md)

    **Creating a new flow log that tracks the traffic of your Amazon Redshift cluster** is incorrect because, by default, you cannot create a flow log for your Amazon Redshift cluster. You have to enable Enhanced VPC Routing and set up the required VPC configuration.

A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. Which of the following is the most suitable solution that you should implement to meet the requirement?

- ​Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and a Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using SNS.
- ​Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email.
- ​Using the Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using SNS.
- ​Create a Lambda function that uses DynamoDB Streams Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an SNS Topic that will notify the subscribers via email when there is an update made by a particular user.
- Answer

    A **DynamoDB stream** is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.

    Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A *stream record* contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items.

    Amazon DynamoDB is integrated with AWS Lambda so that you can create *triggers*—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.

    If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow.

    Hence, the correct answer in this scenario is the option that says: **Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email**.

    ![https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/StreamsAndTriggers.png](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/StreamsAndTriggers.png)

    The option that says: **Using the Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using SNS** is incorrect because although this is a valid solution, it is missing a vital step which is to enable DynamoDB Streams. With the DynamoDB Streams Kinesis Adapter in place, you can begin developing applications via the KCL interface, with the API calls seamlessly directed at the DynamoDB Streams endpoint. Remember that the DynamoDB Stream feature is not enabled by default.

    The option that says: **Create a Lambda function that uses DynamoDB Streams Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an SNS Topic that will notify the subscribers via email when there is an update made by a particular user** is incorrect because just like in the above, you have to manually enable DynamoDB Streams first before you can use its endpoint.

    The option that says: **Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and a Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using SNS** is incorrect because the DynamoDB Accelerator (DAX) feature is primarily used to significantly improve the in-memory read performance of your database, and not to capture the time-ordered sequence of item-level modifications. You should use DynamoDB Streams in this scenario instead.

    **References:**

    [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html)

    [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html)

    **Check out this Amazon DynamoDB Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/)

**A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently. Which of the following would you consider to implement for your DynamoDB table?**

- ​Reduce the number of partition keys in the DynamoDB table.
- ​Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.
- ​Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.
- ​Avoid using a composite primary key, which is composed of a partition key and a sort key
- Answer

    The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This in turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create "hot" partitions that result in throttling and use your provisioned I/O capacity inefficiently.

    The optimal usage of a table's provisioned throughput depends not only on the workload patterns of individual items, but also on the partition-key design. This doesn't mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases.

    One example for this is the use of **partition keys with high-cardinality attributes, which have a large number of distinct values for each item**.

    **Reducing the number of partition keys in the DynamoDB table** is incorrect because instead of doing this, you should actually add more to improve its performance to distribute the I/O requests evenly and not avoid "hot" partitions.

    **Using partition keys with low-cardinality attributes, which have a few number of distinct values for each item** is incorrect because this is the exact opposite of the correct answer. Remember that the more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, the less distinct partition key values, the less evenly spread it would be across the partitioned space, which effectively slows the performance.

    The option that says: **Avoid using a composite primary key, which is composed of a partition key and a sort key** is incorrect because as mentioned, a composite primary key will provide more partition for the table and in turn, improves the performance. Hence, it should be used and not avoided.

    **References:**

    [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html)

    [https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/](https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/)

    **Check out this Amazon DynamoDB Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/)

**In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS. What are the features in AWS that can ensure data security for your confidential documents? (Select TWO.)**

- [ ]  ​S3 Server-Side Encryption
- [ ]  ​Public Data Set Volume Encryption
- [ ]  ​S3 On-Premises Data Encryption
- [ ]  ​S3 Client-Side Encryption
- [ ]  ​EBS On-Premises Data Encryption
- Answer

    You can secure the privacy of your data in AWS, both at rest and in-transit, through encryption. If your data is stored in EBS Volumes, you can enable EBS Encryption and if it is stored on Amazon S3, you can enable **client-side** and **server-side encryption**.

    **Public Data Set Volume Encryption** is incorrect as public data sets are designed to be publicly accessible.

    **EBS On-Premises Data Encryption** and **S3 On-Premises Data Encryption** are both incorrect as there is no such thing as On-Premises Data Encryption for S3 and EBS as these services are in the AWS cloud and not on your on-premises network.

**A travel photo sharing website is using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business. What is the MOST effective method to mitigate this issue?**

- ​Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.
- ​Block the IP addresses of the offending websites using NACL.
- ​Use CloudFront distributions for your photos.
- ​Store and privately serve the high-quality photos on Amazon WorkDocs instead.
- Answer

    In Amazon S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.

    When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.

    Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-58-48-19fb174d579477b46d422a9b264f6455.jpg](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-58-48-19fb174d579477b46d422a9b264f6455.jpg)

    ***Using CloudFront distributions for your photos*** is incorrect. CloudFront is a content delivery network service that speeds up delivery of content to your customers.

    ***Blocking the IP addresses of the offending websites using NACL*** is also incorrect. Blocking IP address using NACLs is not a very efficient method because a quick change in IP address would easily bypass this configuration.

    ***Storing and privately serving the high-quality photos on Amazon WorkDocs instead*** is incorrect as WorkDocs is simply a fully managed, secure content creation, storage, and collaboration service. It is not a suitable service for storing static content. Amazon WorkDocs is more often used to easily create, edit, and share documents for collaboration and not for serving object data like Amazon S3.

    **References:**

    [https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html)

    [https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectOperations.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectOperations.html)

    **Check out this Amazon CloudFront Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/)

    **S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)**

    [https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/](https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/)

    **Comparison of AWS Services Cheat Sheets:**

    [https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/](https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/)

**A Solutions Architect is tasked to host a web application in a new VPC with private and public subnets. In order to do this, the Architect will need to deploy a new MySQL database server and a fleet of EC2 instances to host the application. In which subnet should the Architect launch the new database server into?**

- ​The private subnet
- ​Ideally be launched outside the Amazon VPC
- ​The public subnet
- ​Either public or private subnet
- Answer

    In an ideal and secure VPC architecture, you launch the web servers or elastic load balancers in the public subnet and the database servers in the private subnet.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-53-23-254fea6a886ca730a6b094cc1f7746f4.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-53-23-254fea6a886ca730a6b094cc1f7746f4.png)

    **The private subnet** is correct because it is more secure to launch your database in the private subnet to prevent other external and unauthorized users to access or attack your system.

    **The public subnet** is incorrect because if you launch your database server in the public subnet, it will be publicly accessible all over the Internet which has a higher security risk.

    **Either public or private subnet** is incorrect since only the private subnet is the correct answer if you want to secure your database from external traffic.

    The option that says: **Ideally be launched outside the Amazon VPC** is incorrect as there is no need to launch it outside the VPC. Having it run in a private subnet should address the security and networking concerns of your database.

**You founded a tech startup that provides online training and software development courses to various students across the globe. Your team has developed an online portal in AWS where the students can log into and access the courses they are subscribed to. Since you are in the early phases of the startup and the funding is still hard to come by, which service can help you manage the budgets for all your AWS resources?**

- ​Cost Allocation Tags
- ​Payment History
- ​Cost Explorer
- ​AWS Budgets
- Answer

    AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.

    Budgets can be tracked at the monthly, quarterly, or yearly level, and you can customize the start and end dates. You can further refine your budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others. Budget alerts can be sent via email and/or Amazon Simple Notification Service (SNS) topic.

    You can also use AWS Budgets to set a custom reservation utilization target and receive alerts when your utilization drops below the threshold you define. RI utilization alerts support Amazon EC2, Amazon RDS, Amazon Redshift, and Amazon ElastiCache reservations.

    Budgets can be created and tracked from the AWS Budgets dashboard or via the Budgets API.

    **Cost Explorer** is incorrect because it only helps you visualize and manage your AWS costs and usages over time. It offers a set of reports you can view data with for up to the last 13 months, forecast how much you're likely to spend for the next three months, and get recommendations for what Reserved Instances to purchase. You use Cost Explorer to identify areas that need further inquiry and see trends to understand your costs.

    **Cost Allocation Tags** is incorrect because it only eases the organization of your resource costs on your cost allocation report to make it easier for you to categorize and track your AWS costs.

    **Payment History** is incorrect because this option only provides a location where you can view the monthly invoices you receive from AWS. If your account isn't past due, the Payment History page shows only previous invoices and payment status.

⭐️

**You are managing a suite of applications in your on-premises network which are using trusted IP addresses that your partners and customers have whitelisted in their firewalls. There is a requirement to migrate these applications to AWS without requiring your partners and customers to change their IP address whitelists.** 

**Which of the following is the most suitable solution to properly migrate your applications?**

- ​Submit an AWS Request Form to migrate the IP address range that you own to your AWS Account.
- ​Set up a list of Elastic IP addresses to map the whitelisted IP address range in your on-premises network.
- ​Create a Route Origin Authorization (ROA) then once done, provision and advertise your whitelisted IP address range to your AWS account.
- ​Set up an IP match condition using a CloudFront web distribution and AWS WAF to whitelist a specific IP address range in your VPC.
- Answer

    You can bring part or all of your public IPv4 address range from your on-premises network to your AWS account. You continue to own the address range, but AWS advertises it on the Internet. After you bring the address range to AWS, it appears in your account as an address pool. You can create an Elastic IP address from your address pool and use it with your AWS resources, such as EC2 instances, NAT gateways, and Network Load Balancers. This is also called "Bring Your Own IP Addresses (BYOIP)".

    To ensure that only you can bring your address range to your AWS account, you must authorize Amazon to advertise the address range and provide proof that you own the address range.

    A **Route Origin Authorization (ROA)** is a document that you can create through your Regional internet registry (RIR), such as the American Registry for Internet Numbers (ARIN) or Réseaux IP Européens Network Coordination Centre (RIPE). It contains the address range, the ASNs that are allowed to advertise the address range, and an expiration date. Hence, Option 3 is the correct answer.

    The ROA authorizes Amazon to advertise an address range under a specific AS number. However, it does not authorize your AWS account to bring the address range to AWS. To authorize your AWS account to bring an address range to AWS, you must publish a self-signed X509 certificate in the RDAP remarks for the address range. The certificate contains a public key, which AWS uses to verify the authorization-context signature that you provide. You should keep your private key secure and use it to sign the authorization-context message.

    **Setting up a list of Elastic IP addresses to map the whitelisted IP address range in your on-premises network** is incorrect because you cannot map the IP address of your on-premises network, which you are migrating to AWS, to an EIP address of your VPC. To satisfy the requirement, you must authorize Amazon to advertise the address range that you own.

    **Setting up an IP match condition using a CloudFront web distribution and AWS WAF to whitelist a specific IP address range in your VPC** is incorrect because the IP match condition in CloudFront is primarily used in allowing or blocking the incoming web requests based on the IP addresses that the requests originate from. This is the opposite of what is being asked in the scenario, where you have to migrate your suite of applications from your on-premises network and advertise the address range that you own in your VPC.

    **Submitting an AWS Request Form to migrate the IP address range that you own to your AWS Account** is incorrect because you don't need to submit an AWS request in order to do this. You can simply create a Route Origin Authorization (ROA) then once done, provision and advertise your whitelisted IP address range to your AWS account.

**You are a Solutions Architect in your company working with 3 DevOps Engineers under you. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service. What can you do to prevent this from happening again?**

- ​Use S3 Infrequently Accessed storage to store the data.
- ​Set up a signed URL for all users.
- ​Create an IAM bucket policy that disables delete operation.
- ​Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.
- Answer

    To avoid accidental deletion in Amazon S3 bucket, you can:

    - Enable Versioning

    - Enable MFA (Multi-Factor Authentication) Delete

**You are designing a multi-tier web application architecture that consists of a fleet of EC2 instances and an Oracle relational database server. It is required that the database is highly available and that you have full control over its underlying operating system. Which AWS service will you use for your database tier?**

- ​Amazon RDS
- ​Amazon EC2 instances with data replication between two different Availability Zones
- ​Amazon EC2 instances with data replication in one Availability Zone
- ​Amazon RDS with Multi-AZ deployments
- Answer

    To achieve this requirement, you can deploy your Oracle database to **Amazon EC2 instances with data replication between two different Availability Zones**. Hence, option 4 is the correct answer. The deployment of this architecture can easily be achieved by using Cloudformation and Quick Start. 

    The Quick Start deploys the Oracle primary database (using the preconfigured, general-purpose starter database from Oracle) on an Amazon EC2 instance in the first Availability Zone. It then sets up a second EC2 instance in a second Availability Zone, copies the primary database to the second instance by using the `DUPLICATE` command, and configures Oracle Data Guard.

    **Amazon RDS** and **Amazon RDS with Multi-AZ deployments** are both incorrect because the scenario requires you to have access to the underlying operating system of the database server. Remember that Amazon RDS is a managed database service, which means that Amazon is the one that manages the underlying operating system of the database instance and not you.

    The option that says: **Amazon EC2 instances with data replication in one Availability Zone** is incorrect since deploying to just one Availability Zone (AZ) will not make the database tier highly available. If that AZ went down, your database will be unavailable.

**A suite of web applications is hosted in an Auto Scaling group of EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application. Which EC2 instance will be the first one to be terminated by your Auto Scaling group?**

- ​The instance will be randomly selected by the Auto Scaling group
- ​The EC2 instance launched from the oldest launch configuration
- ​The EC2 instance which has been running for the longest time
- ​The EC2 instance which has the least number of user sessions
- Answer

    The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:

    1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.

    2. Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.

    3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.

    4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.

    The following flow diagram illustrates how the default termination policy works:

    ![https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/termination-policy-default-flowchart-diagram.png](https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/termination-policy-default-flowchart-diagram.png)

**A Solutions Architect is hosting a website in an Amazon S3 bucket named `tutorialsdojo`. The users load the website using the following URL: `http://tutorialsdojo.s3-website-us-east-1.amazonaws.com` and there is a new requirement to add a JavaScript on the webpages in order to make authenticated HTTP `GET` requests against the same bucket by using the Amazon S3 API endpoint (`tutorialsdojo.s3.amazonaws.com`). Upon testing, you noticed that the web browser blocks JavaScript from allowing those requests. Which of the following options is the MOST suitable solution that you should implement for this scenario?**

- ​Enable Cross-Region Replication (CRR).
- ​Enable Cross-Zone Load Balancing.
- ​Enable Cross-origin resource sharing (CORS) configuration in the bucket.
- ​Enable cross-account access.
- Answer

    Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.

    ![https://docs.amazonaws.cn/en_us/sdk-for-javascript/v2/developer-guide/images/cors-overview.png](https://docs.amazonaws.cn/en_us/sdk-for-javascript/v2/developer-guide/images/cors-overview.png)

    Suppose that you are hosting a website in an Amazon S3 bucket named `your-website` and your users load the website endpoint `http://your-website.s3-website-us-east-1.amazonaws.com`. Now you want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket, `your-website.s3.amazonaws.com`. A browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to explicitly enable cross-origin requests from `your-website.s3-website-us-east-1.amazonaws.com`.

    In this scenario, you can solve the issue by enabling the CORS in the S3 bucket. Hence, **enabling Cross-origin resource sharing (CORS) configuration in the bucket** is the correct answer.

    **Enabling cross-account access** is incorrect because cross-account access is a feature in IAM and not in Amazon S3.

    **Enabling Cross-Zone Load Balancing** is incorrect because Cross-Zone Load Balancing is only used in ELB and not in S3.

    **Enabling Cross-Region Replication (CRR)** is incorrect because CRR is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.

    **References:**

    [http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html](http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html)

    [https://docs.aws.amazon.com/AmazonS3/latest/dev/ManageCorsUsing.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/ManageCorsUsing.html)

**A retail website has intermittent, sporadic, and unpredictable transactional workloads throughout the day that are hard to predict. The website is currently hosted on-premises and is slated to be migrated to AWS. A new relational database is needed that autoscales capacity to meet the needs of the application's peak load and scales back down when the surge of activity is over. Which of the following option is the MOST cost-effective and suitable database setup in this scenario?**

- ​Launch an Amazon Aurora Serverless DB cluster then set the minimum anœœd maximum capacity for the cluster.
- ​Launch a DynamoDB Global table with Auto Scaling enabled.
- ​Launch an Amazon Aurora Provisioned DB cluster with burstable performance DB instance class types.
- ​Launch an Amazon Redshift data warehouse cluster with Concurrency Scaling.
- Answer

    Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. **An Aurora Serverless DB cluster is a DB cluster that automatically starts up, shuts down, and scales up or down its compute capacity based on your application's needs**. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, sporadic or unpredictable workloads. It can provide this because it automatically starts up, scales compute capacity to match your application's usage and shuts down when it's not in use.

    Take note that a **non-Serverless DB cluster for Aurora is called a provisioned DB cluster.** Aurora Serverless clusters and provisioned clusters both have the same kind of high-capacity, distributed, and highly available storage volume.

    When you work with Amazon Aurora without Aurora Serverless (provisioned DB clusters), you can choose your DB instance class size and create Aurora Replicas to increase read throughput. If your workload changes, you can modify the DB instance class size and change the number of Aurora Replicas. **This model works well when the database workload is predictable, because you can adjust capacity manually based on the expected workload.**

    However, in some environments, workloads can be intermittent and unpredictable. There can be periods of heavy workloads that might last only a few minutes or hours, and also long periods of light activity, or even no activity. Some examples are retail websites with intermittent sales events, reporting databases that produce reports when needed, development and testing environments, and new applications with uncertain requirements. In these cases and many others, it can be difficult to configure the correct capacity at the right times. It can also result in higher costs when you pay for capacity that isn't used.

    ![https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/images/aurora-serverless-arch.png](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/images/aurora-serverless-arch.png)

    With Aurora Serverless , you can create a database endpoint without specifying the DB instance class size. You set the minimum and maximum capacity. With Aurora Serverless, the database endpoint connects to a *proxy fleet* that routes the workload to a fleet of resources that are automatically scaled. Because of the proxy fleet, connections are continuous as Aurora Serverless scales the resources automatically based on the minimum and maximum capacity specifications. Database client applications don't need to change to use the proxy fleet. Aurora Serverless manages the connections automatically. Scaling is rapid because it uses a pool of "warm" resources that are always ready to service requests. Storage and processing are separate, so you can scale down to zero processing and pay only for storage.

    Aurora Serverless introduces a new `serverless` DB engine mode for Aurora DB clusters. Non-Serverless DB clusters use the `provisioned` DB engine mode.

    Hence, the correct answer is: ***Launch an Amazon Aurora Serverless DB cluster then set the minimum and maximum capacity for the cluster*.**

    The option that says: ***Launch an Amazon Aurora Provisioned DB cluster then set the minimum and maximum capacity for the cluster*** is incorrect because an Aurora Provisioned DB cluster is not suitable for intermittent, sporadic, and unpredictable transactional workloads. This model works well when the database workload is predictable because you can adjust capacity manually based on the expected workload. A better database setup here is to use an Amazon Aurora Serverless cluster.

    The option that says: ***Launch a DynamoDB Global table with Auto Scaling enabled*** is incorrect because although it is using Auto Scaling, the scenario explicitly indicated that you need a relational database to handle your transactional workloads. DynamoDB is a NoSQL database and is not suitable for this use case. Moreover, the use of a DynamoDB Global table is not warranted since this is primarily used if you need a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications.

    The option that says: ***Launch an Amazon Redshift data warehouse cluster with Concurrency Scaling*** is incorrect because this type of database is primarily used for online analytical processing (OLAP) and not for online transactional processing (OLTP). Concurrency Scaling is simply an Amazon Redshift feature that automatically and elastically scales query processing power of your Redshift cluster to provide consistently fast performance for hundreds of concurrent queries.

    **References:**

    [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html)

    [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html)

**An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct the write operations of the production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances. Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?**

- ​Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.
- ​Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.
- ​Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances.
- ​In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries.
- Answer

    **Amazon Aurora** typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an *endpoint*. 

    ![Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled.png](Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled.png)

    Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available.

    For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-03_00-40-18-850e58ca7802358579be53729bfc0cbb.gif](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-03_00-40-18-850e58ca7802358579be53729bfc0cbb.gif)

    Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.

    The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances. Hence, **creating a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries** is the correct answer.

    **Configuring your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas** is incorrect because although it is true that a reader endpoint enables your Aurora database to automatically perform load-balancing among all the Aurora Replicas, it is quite limited to doing read operations only. You still need to use a custom endpoint to load-balance the database connections based on the specified criteria.

    The option that says: **In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries** is incorrect because a cluster endpoint (also known as a writer endpoint) for an Aurora DB cluster simply connects to the current primary DB instance for that DB cluster. This endpoint can perform write operations in the database such as DDL statements, which is perfect for handling production traffic but not suitable for handling queries for reporting since there will be no write database operations that will be sent. Moreover, the endpoint does not point to lower-capacity or high-capacity instances as per the requirement. A better solution for this is to use a custom endpoint.

    The option that says: **Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances** is incorrect because Aurora does not do this by default. You have to create custom endpoints in order to accomplish this requirement.

    **Reference**:

    [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html)

    **Check out this Amazon Aurora Cheat Sheet:**

    [https://tutorialsdojo.com/amazon-aurora/](https://tutorialsdojo.com/amazon-aurora/)

**You are leading a software development team which uses serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. You have a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform and also uses a third party API to fetch certain data for your application. You instructed one of your junior developers to create the environment variables for the MongoDB database hostname, username, and password as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT and PROD environments.** 

**Considering that the Lambda function is storing sensitive database and API credentials, how can you secure this information to prevent other developers in your team, or anyone, from seeing these credentials in plain text? Select the best option that provides the maximum security.**

- ​Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information.
- ​AWS Lambda does not provide encryption for the environment variables. Deploy your code to an EC2 instance instead.
- ​Create a new KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.
- ​There is no need to do anything because, by default, AWS Lambda already encrypts the environment variables using the AWS Key Management Service.
- Answer

    When you create or update Lambda functions that use environment variables, AWS Lambda encrypts them using the AWS Key Management Service. When your Lambda function is invoked, those values are decrypted and made available to the Lambda code.

    The first time you create or update Lambda functions that use environment variables in a region, a default service key is created for you automatically within AWS KMS. This key is used to encrypt environment variables. However, if you wish to use encryption helpers and use KMS to encrypt environment variables after your Lambda function is created, you must create your own AWS KMS key and choose it instead of the default key. The default key will give errors when chosen. Creating your own key gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data.

    ![https://i.udemycdn.com/redactor/raw/2019-07-26_21-22-58-bc110e8ef8d91e32af6af839a0296911.png](https://i.udemycdn.com/redactor/raw/2019-07-26_21-22-58-bc110e8ef8d91e32af6af839a0296911.png)

    The option that says: **There is no need to do anything because, by default, AWS Lambda already encrypts the environment variables using the AWS Key Management Service** is incorrect because although Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables.

    The option that says: **Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information** is also incorrect since enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Use AWS KMS instead.

    The option that says: **AWS Lambda does not provide encryption for the environment variables. Deploy your code to an EC2 instance instead** is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.

    **References:**

    [https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html#env_encrypt](https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html#env_encrypt)

    [https://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_console.html](https://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_console.html)

    **Check out this AWS Lambda Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/](https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/)

**[SAA-C02] A global IT company with offices around the world has multiple AWS accounts. To improve efficiency and drive costs down, the Chief Information Officer (CIO) wants to set up a solution that centrally manages their AWS resources. This will allow them to procure AWS resources centrally and share resources such as AWS Transit Gateways, AWS License Manager configurations, or Amazon Route 53 Resolver rules across their various accounts. As the Solutions Architect, which combination of options should you implement in this scenario? (Select TWO.)**

- [ ]  ​Use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts.
- [ ]  ​Consolidate all of the company's accounts using AWS Organizations.
- [ ]  ​Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.
- [ ]  ​Use AWS Control Tower to easily and securely share your resources with your AWS accounts.
- [ ]  ​Consolidate all of the company's accounts using AWS ParallelCluster.
- Answer

    AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. **You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM.**

    Many organizations use multiple accounts to create administrative or billing isolation, and limit the impact of errors. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge.

    ![https://d1.awsstatic.com/products/RAM/product-page-diagram_AWS-Resource-Access-Manager(1).379df75d48a8e2cc6160859b7ca3626a9b9be0c1.png](https://d1.awsstatic.com/products/RAM/product-page-diagram_AWS-Resource-Access-Manager(1).379df75d48a8e2cc6160859b7ca3626a9b9be0c1.png)

    You can procure AWS resources centrally, and use RAM to share resources such as subnets or License Manager configurations with other accounts. This eliminates the need to provision duplicate resources in every account in a multi-account environment, reducing the operational overhead of managing those resources in every account.

    AWS Organizations is an account management service that lets you consolidate multiple AWS accounts into an organization that you create and centrally manage. With Organizations, you can create member accounts and invite existing accounts to join your organization. You can organize those accounts into groups and attach policy-based controls.

    Hence, the correct combination of options in this scenario is:

    **- *Consolidate all of the company's accounts using AWS Organizations.***

    ***- Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.***

    The option that says: ***Use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts*** is incorrect because although you can delegate access to resources that are in different AWS accounts using IAM, this process is extremely tedious and entails a lot of operational overhead since you have to manually set up cross-account access to each and every AWS account of the company. A better solution is to use AWS Resources Access Manager instead.

    The option that says: ***Use AWS Control Tower to easily and securely share your resources with your AWS accounts*** is incorrect because AWS Control Tower simply offers the easiest way to set up and govern a new, secure, multi-account AWS environment. This is not the most suitable service to use to securely share your resources across AWS accounts or within your Organization. You have to use AWS Resources Access Manager (RAM) instead.

    The option that says: ***Consolidate all of the company's accounts using AWS ParallelCluster*** is incorrect because AWS ParallelCluster is simply an AWS-supported open-source cluster management tool that makes it easy for you to deploy and manage High-Performance Computing (HPC) clusters on AWS. In this particular scenario, it is more appropriate to use AWS Organizations to consolidate all of your AWS accounts.

**You are working as a Solutions Architect for a technology company which is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and can handle frequent schema changes. The application should not have any downtime or performance issues whenever there is a schema change in the database. It should also provide low-latency response to high-traffic queries.** 

**Which is the most suitable database solution to use to achieve this requirement?**

- ​Amazon DynamoDB
- ​Redshift
- ​An Amazon Aurora database with Read Replicas
- ​An Amazon RDS instance in Multi-AZ Deployments configuration
- Answer

    Before we proceed in answering this question, we must first be clear with the actual definition of a "**schema**". Basically, the english definition of a schema is: *a representation of a plan or theory in the form of an outline or model*.

    Just think of a schema as the "structure" or a "model" of your data in your database. Since the scenario requires that the schema, or the structure of your data, changes frequently, then you have to pick a database which provides a non-rigid and flexible way of adding or removing new types of data. This is a classic example of choosing between a relational database and non-relational (NoSQL) database.

    A relational database is known for having a rigid schema, with a lot of constraints and limits as to which (and what type of ) data can be inserted or not. It is primarily used for scenarios where you have to support complex queries which fetch data across a number of tables. It is best for scenarios where you have complex table relationships but for use cases where you need to have a flexible schema, this is not a suitable database to use.

    For NoSQL, it is not as rigid as a relational database because you can easily add or remove rows or elements in your table/collection entry. It also has a more flexible schema because it can store complex hierarchical data within a single item which, unlike a relational database, does not entail changing multiple related tables. Hence, the best answer to be used here is a NoSQL database, like DynamoDB. When your business requires a low-latency response to high-traffic queries, taking advantage of a NoSQL system generally makes technical and economic sense.

    Amazon DynamoDB helps solve the problems that limit the relational system scalability by avoiding them. In DynamoDB, you design your schema specifically to make the most common and important queries as fast and as inexpensive as possible. Your data structures are tailored to the specific requirements of your business use cases.

    Remember that a relational database system **does not scale** well for the following reasons:

    - It normalizes data and stores it on multiple tables that require multiple queries to write to disk.

    - It generally incurs the performance costs of an ACID-compliant transaction system.

    - It uses expensive joins to reassemble required views of query results.

    For DynamoDB, it scales well due to these reasons:

    - Its **schema flexibility** lets DynamoDB store complex hierarchical data within a single item. DynamoDB is not a totally *schemaless* database since the very definition of a schema is just the model or structure of your data.

    - Composite key design lets it store related items close together on the same table.

    **An Amazon RDS instance in Multi-AZ Deployments configuration** and **an Amazon Aurora database with Read Replicas** are incorrect because both of them are a type of relational database.

    **Redshift** is incorrect because it is primarily used for OLAP systems.

**There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?**

- ​Increase the database instance size
- ​Create a read replica
- ​Enabled Multi-AZ failover
- ​Make a snapshot of the database
- Answer

    Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. For this scenario, enabling Multi-AZ failover is the correct answer. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.

    ![https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png)

    In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.

    **Making a snapshot of the database** allows you to have a backup of your database, but it does not provide immediate availability in case of AZ failure. So this is incorrect.

    **Increasing the database instance size** is not a solution for this problem. Doing this action addresses the need to upgrade your compute capacity but does not solve the requirement of providing access to your database even in the event of a loss of one of the Availability Zones.

    **Creating a read replica** is incorrect because this simply provides enhanced performance for read-heavy database workloads. Although you can promote a read replica, its asynchronous replication might not provide you the latest version of your database.

    **Reference:**

    [https://aws.amazon.com/rds/details/multi-az/](https://aws.amazon.com/rds/details/multi-az/)

    **Check out this Amazon RDS Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/)

    **Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:**

    [https://tutorialsdojo.com/aws-cheat-sheet-aws-certified-solutions-architect-associate/](https://tutorialsdojo.com/aws-cheat-sheet-aws-certified-solutions-architect-associate/)

**An online cryptocurrency exchange platform is hosted in AWS which uses ECS Cluster and RDS in Multi-AZ Deployments configuration. The application is heavily using the RDS instance to process complex read and write database operations. To maintain the reliability, availability, and performance of your systems, you have to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of the CPU bandwidth and total memory consumed by each process. Which of the following is the most suitable solution to properly monitor your database?**

- ​Create a script that collects and publishes custom metrics to CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.
- ​Enable Enhanced Monitoring in RDS.
- ​Use Amazon CloudWatch to monitor the CPU Utilization of your database.
- ​Check the `CPU%` and `MEM%` metrics which are readily available in the Amazon RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance.
- Answer

    Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console, or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the `RDSOSMetrics` log group in the CloudWatch console.

    Take note that there are certain differences between CloudWatch and Enhanced Monitoring Metrics. **CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance**. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work. Hence, **enabling Enhanced Monitoring in RDS** is the correct answer in this specific scenario.

    The differences can be greater if your DB instances use smaller instance classes, because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.

    ![Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%201.png](Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%201.png)

    **Using Amazon CloudWatch to monitor the CPU Utilization of your database** is incorrect because although you can use this to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.

    The option that says: **Create a script that collects and publishes custom metrics to CloudWatch, which tracks the real-time CPU Utilization of the RDS instance and then set up a custom CloudWatch dashboard to view the metrics** is incorrect because although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is still not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database processes. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance.

    The option that says: **Check the `CPU%` and `MEM%` metrics which are readily available in the Amazon RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance** is incorrect because the CPU% and MEM% metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option.

    Log in each process → Enhanced Monitoring in RDS

    **References:**

    [https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs)

    [https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch)

    **Check out this Amazon CloudWatch Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/)

    **Check out this Amazon RDS Cheat Sheet:**

    **[https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/)**

**A popular mobile game uses CloudFront, Lambda, and DynamoDB for its backend services. The player data is persisted on a DynamoDB table and the static assets are distributed by CloudFront. However, there are a lot of complaints that saving and retrieving player information is taking a lot of time. To improve the game's performance, which AWS service can you use to reduce DynamoDB response times from milliseconds to microseconds?**

- ​AWS Device Farm
- ​Amazon ElastiCache
- ​DynamoDB Auto Scaling
- ​Amazon DynamoDB Accelerator (DAX)
- Answer

    Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, **in-memory cache** that can reduce Amazon DynamoDB response times from milliseconds to microseconds, even at millions of requests per second.

    ![Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%202.png](Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%202.png)

    **Amazon ElastiCache** is incorrect because although you may use ElastiCache as your database cache, it will not reduce the DynamoDB response time from milliseconds to microseconds as compared with DynamoDB DAX.

    **AWS Device Farm** is incorrect because this is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time.

    **DynamoDB Auto Scaling** is incorrect because this is primarily used to automate capacity management for your tables and global secondary indexes.

    **References:**

    [https://aws.amazon.com/dynamodb/dax](https://aws.amazon.com/dynamodb/dax)[https://aws.amazon.com/device-farm](https://aws.amazon.com/device-farm)

    **Check out this Amazon DynamoDB Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/)

**A government entity is conducting a population and housing census in the city. Each household information uploaded on their online portal is stored in encrypted files in Amazon S3. The government assigned its Solutions Architect to set compliance policies that verify sensitive data in a manner that meets their compliance standards. They should also be alerted if there are compromised files detected containing personally identifiable information (PII), protected health information (PHI) or intellectual properties (IP). Which of the following should the Architect implement to satisfy this requirement?**

- ​Set up and configure Amazon GuardDuty to monitor malicious activity on their Amazon S3 data.
- ​Set up and configure Amazon Rekognition to monitor and recognize patterns on their Amazon S3 data.
- ​Set up and configure Amazon Macie to monitor and detect usage patterns on their Amazon S3 data.
- ​Set up and configure Amazon Inspector to send out alert notifications whenever a security violation is detected on their Amazon S3 data.
- Answer

    **Amazon Macie** is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.

    Amazon Macie continuously monitors data access activity for anomalies, and delivers alerts when it detects risk of unauthorized access or inadvertent data leaks. Amazon Macie has ability to detect global access permissions inadvertently being set on sensitive data, detect uploading of API keys inside source code, and verify sensitive customer data is being stored and accessed in a manner that meets their compliance standards.

    ![https://d1.awsstatic.com/security-center/MacieAlerts.e24f053b850b5c67785134a96718b568740fe250.jpg](https://d1.awsstatic.com/security-center/MacieAlerts.e24f053b850b5c67785134a96718b568740fe250.jpg)

    Hence, the correct answer is: ***Set up and configure Amazon Macie to monitor and detect usage patterns on their Amazon S3 data*.**

    The option that says: ***Set up and configure Amazon Rekognition to monitor and recognize patterns on their Amazon S3 data*** is incorrect because Rekognition is simply a service that can identify the objects, people, text, scenes, and activities, as well as detect any inappropriate content on your images or videos.

    The option that says: ***Set up and configure Amazon GuardDuty to monitor malicious activity on their Amazon S3 data*** is incorrect because GuardDuty is just a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.

    The option that says: ***Set up and configure Amazon Inspector to send out alert notifications whenever a security violation is detected on their Amazon S3 data*** is incorrect because Inspector is basically an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.

    **Check out this Amazon Macie Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-macie/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-macie/)

**You have launched a new enterprise application with a web server and a database. You are using a large EC2 Instance with one 500 GB EBS volume to host a relational database. Upon checking the performance, it shows that write throughput to the database needs to be improved. Which of the following is the most suitable configuration to help you achieve this requirement? (Select TWO.)**

- [ ]  ​Set up the EC2 instance in a placement group
- [ ]  ​Re-launch the instance with a Paravirtual (PV) AMI and enable Enhanced Networking
- [ ]  ​Set up a standard RAID 0 configuration with two EBS Volumes
- [ ]  ​Increase the size of the EC2 Instance
- [ ]  ​Use a standard RAID 1 configuration with two EBS Volumes
- Answer

    The goal here is to increase the write performance of the database hosted in an EC2 instance. You can achieve this by either setting up a standard RAID 0 configuration or simply by increasing the size of the EC2 instance.

    Some EC2 instance types can drive more I/O throughput than what you can provision for a single EBS volume. You can join multiple `gp2`, `io1`, `st1`, or `sc1` volumes together in a RAID 0 configuration to use the available bandwidth for these instances.

    ![https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/ebs_backed_instance.png](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/ebs_backed_instance.png)

    With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level. For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together.

    Take note that HVM AMIs are required to take advantage of enhanced networking and GPU processing. In order to pass through instructions to specialized network and GPU devices, the OS needs to be able to have access to the native hardware platform which the HVM virtualization provides.

    **Re-launching the instance with a Paravirtual (PV) AMI and enabling Enhanced Networking** is incorrect because although the Enhanced Networking feature can provide higher I/O performance and lower CPU utilization to your EC2 instance, you have to use an HVM AMI instead of PV AMI.

    **Using a standard RAID 1 configuration with two EBS Volumes** is incorrect because the main use case for RAID 1 is to provide mirroring, redundancy, and fault-tolerance. RAID 0 is a more suitable option for providing faster read and write operations, compared with RAID 1.

    **Setting up the EC2 instance in a placement group** is incorrect because the placement groups feature is primarily used for inter-instance communication.

    [EC2, EBS, AMI, CloudWatch, EFS, WAF, Placement Groups](../EC2%20EBS%20AMI%20CloudWatch%20EFS%20WAF%20Placement%20Groups%20639bcc497b23431fb99b65f79d411519.md)

    **Check out this Amazon EC2 Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/](https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/)

**A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances. The application is extensively used during office hours from 9 in the morning till 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours. Which of the following can be done to ensure that the application works properly at the beginning of the day?**

- ​Set up an Application Load Balancer (ALB) to your architecture to ensure that the traffic is properly distributed on the instances.
- ​Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day.
- ​Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization.
- ​Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization.
- Answer

    Scaling based on a schedule allows you to scale your application in response to predictable load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling activities based on the predictable traffic patterns of your web application.

    ![https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-sample-web-architecture-diagram-with-asgs.png](https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-sample-web-architecture-diagram-with-asgs.png)

    To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size specified by the scaling action. You can create scheduled actions for scaling one time only or for scaling on a recurring schedule.

    Hence, **configuring a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day** is the correct answer. You need to configure a Scheduled scaling policy. This will ensure that the instances are already scaled up and ready before the start of the day since this is when the application is used the most.

    **Configuring a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization** and **configuring a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization** are both incorrect because although these are valid solutions, it is still better to configure a Scheduled scaling policy as you already know the exact peak hours of your application. By the time either the CPU or Memory hits a peak, the application already has performance issues, so you need to ensure the scaling is done beforehand using a Scheduled scaling policy.

    **Setting up an Application Load Balancer (ALB) to your architecture to ensure that the traffic is properly distributed on the instances** is incorrect. Although the Application load balancer can also balance the traffic, it cannot increase the instances based on demand.

**You are working as a Solutions Architect for a government project in which they are building an online portal to allow people to pay their taxes and claim their tax refunds online. Due to the confidentiality of data, the security policy requires that the application hosted in EC2 encrypts the data first before writing it to the disk for storage. In this scenario, which service would you use to meet this requirement?**

- ​Elastic File System (EFS)
- ​AWS KMS API
- ​EBS encryption
- ​Security Token Service

### **Explanation**

- Answer

    AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. The master keys that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules. AWS KMS is integrated with most other AWS services that encrypt your data with encryption keys that you manage. AWS KMS is also integrated with AWS CloudTrail to provide encryption key usage logs to help meet your auditing, regulatory and compliance needs.

    ![https://docs.aws.amazon.com/kms/latest/developerguide/images/encrypt-with-data-key.png](https://docs.aws.amazon.com/kms/latest/developerguide/images/encrypt-with-data-key.png)

    The scenario mentions that you have to encrypt the data **before** writing it to disk for storage. What this means is that you will have to temporarily store the data in **memory**and not persist it on the disk, then encrypt it on the fly before finally storing it. The end result would be an encrypted data in your disk EBS Volume, and the EBS Encryption would be the secondary layer of protection/encryption for your sensitive data.

    You can configure your application to use the KMS API to encrypt all data before saving it to disk. Hence, **AWS KMS API** is the correct answer.

    **Security Token Service** is incorrect because AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). It is not used for encrypting data unlike KMS.

    **EBS encryption** is incorrect because although EBS encryption provides additional security for the EBS volumes, the application could not use this service to encrypt or decrypt each individual data that it writes on the disk. It is better to use KMS API instead to automatically encrypt the data **before** saving it to disk for maximum security, rather than after.

    **Elastic File System (EFS)** is incorrect because EFS is a storage service and does not provide encryption services unlike KMS API.

    **Check out this AWS Key Management Service (KMS) Cheat Sheet:**

    [https://tutorialsdojo.com/aws-cheat-sheet-aws-key-management-service-aws-kms/](https://tutorialsdojo.com/aws-cheat-sheet-aws-key-management-service-aws-kms/)

**A media company has an Amazon ECS Cluster, which uses the Fargate launch type, to host its news website. The database credentials should be supplied using environment variables, to comply with strict security compliance. As the Solutions Architect, you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself. Which of the following is the most suitable solution in this scenario that you can implement with minimal effort?**

- ​Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt it with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the `--cli-input-json` parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.
- ​Use the AWS Systems Manager Parameter Store to keep the database credentials and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role (`taskRoleArn`) and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.
- ​In the ECS task definition file of the ECS Cluster, store the database credentials using Docker Secrets to centrally manage these sensitive data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest. A given secret is only accessible to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running.
- ​Use the AWS Secrets Manager to store the database credentials and then encrypt them using AWS KMS. Create a resource-based policy for your Amazon ECS task execution role (`taskRoleArn`) and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.
- Answer

    Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.

    Secrets can be exposed to a container in the following ways:

    - To inject sensitive data into your containers as environment variables, use the `secrets` container definition parameter.

    - To reference sensitive information in the log configuration of a container, use the `secretOptions` container definition parameter.

    ![https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/08/25/diagram3-1.png](https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/08/25/diagram3-1.png)

    Within your container definition, specify `secrets` with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account.

    Hence, the correct answer is the option that says: ***Use the AWS Systems Manager Parameter Store to keep the database credentials and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role (`taskRoleArn`) and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container**.*

    The option that says: ***In the ECS task definition file of the ECS Cluster, store the database credentials using Docker Secrets to centrally manage these sensitive data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest. A given secret is only accessible to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running*** is incorrect because although you can use Docker Secrets to secure the sensitive database credentials, this feature is only applicable in Docker Swarm. In AWS, the recommended way to secure sensitive data is either through the use of Secrets Manager or Systems Manager Parameter Store.

    The option that says: ***Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt it with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the `--cli-input-json` parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials*** is incorrect because although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by simply using the Secrets Manager or Systems Manager Parameter Store.

    The option that says: ***Use the AWS Secrets Manager to store the database credentials and then encrypt them using AWS KMS. Create a resource-based policy for your Amazon ECS task execution role (`taskRoleArn`) and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container*** is incorrect because although the use of Secrets Manager in securing sensitive data in ECS is valid, using an IAM Role is a more suitable choice over a resource-based policy for the Amazon ECS task execution role.

**You have a new e-commerce web application written in Angular framework which is deployed to a fleet of EC2 instances behind an Application Load Balancer. You configured the load balancer to perform health checks on these EC2 instances. What will happen if one of these EC2 instances failed the health checks?**

- ​The EC2 instance gets quarantined by the Application Load Balancer for root cause analysis.
- ​The EC2 instance is replaced automatically by the Application Load Balancer.
- ​The Application Load Balancer stops sending traffic to the instance that failed its health check.
- ​The EC2 instance gets terminated automatically by the Application Load Balancer.
- Answer

    In case that one of the EC2 instances failed a health check, the Application Load Balancer stops sending traffic to that instance.

    Your Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called *health checks*. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target group with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check.

**A startup based in Australia is deploying a new two-tier web application in AWS. The Australian company wants to store their most frequently used data in an in-memory data store to improve the retrieval and response time of their web application. Which of the following is the most suitable service to be used for this requirement?**

- ​DynamoDB
- ​Amazon RDS
- ​Amazon ElastiCache
- ​Amazon Redshift
- Answer

    **Amazon ElastiCache** is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.

    ![https://d1.awsstatic.com/elasticache/Databases@2x.2ccaf2321ff1e9c857d9405d62e04de7e087c6ad.png](https://d1.awsstatic.com/elasticache/Databases@2x.2ccaf2321ff1e9c857d9405d62e04de7e087c6ad.png)

    **DynamoDB** is incorrect because this is primarily used as a NoSQL database which supports both document and key-value store models. ElastiCache is a more suitable service to use than DynamoDB, if you need an in-memory data store.

    **Amazon RDS** is incorrect because this is mainly used as a relational database and not as a data storage for frequently used data.

    **Amazon Redshift** is incorrect because this is a data warehouse service and is not suitable to be used as an in-memory data store.

**You have identified a series of DDoS attacks while monitoring your VPC. As the Solutions Architect, you are responsible in fortifying your current cloud infrastructure to protect the data of your clients. Which of the following is the most suitable solution to mitigate these kinds of attacks?**

- ​Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.
- ​Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks and other DDoS attacks.
- ​A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC.
- ​Use AWS Shield to detect and mitigate DDoS attacks.
- Answer

    For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.

    ![https://d1.awsstatic.com/aws-answers/answers-images/web-app-ddos-mitigation.4fc1b1d10a6e8517dcd80547544c1ee4f423da1b.png](https://d1.awsstatic.com/aws-answers/answers-images/web-app-ddos-mitigation.4fc1b1d10a6e8517dcd80547544c1ee4f423da1b.png)

    AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 charges.

    The option that says: **Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks and other DDoS attacks** is incorrect because the AWS Firewall Manager is mainly used to simplify your AWS WAF administration and maintenance tasks across multiple accounts and resources. It does not protect your VPC against DDoS attacks.

    The option that says: **Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic** is incorrect because even though AWS WAF can help you block common attack patterns to your VPC such as SQL injection or cross-site scripting, this is still not enough to withstand DDoS attacks. It is better to use AWS Shield in this scenario.

    The option that says: **A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC** is incorrect because although using a combination of Security Groups and NACLs are valid to provide security to your VPC, this is not enough to mitigate a DDoS attack. You should use AWS Shield for better security protection.

**[SAA-C02] A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory.** 

**In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)**

- [ ]  ​IAM Groups
- [ ]  ​IAM Roles
- [ ]  ​AWS Directory Service Simple AD
- [ ]  ​Lambda
- [ ]  ​AWS Directory Service AD Connector
- Answer

    Considering that the company is using a corporate Active Directory, it is best to use **AWS Directory Service AD Connector** for easier integration. In addition, since the roles are already assigned using groups in the corporate Active Directory, it would be better to also use **IAM Roles**. Take note that you can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector.

    ![https://d1.awsstatic.com/Products/product-name/diagrams/directory_service_howitworks.80bfccbf2f5d1d63558ec3c086aff247147258f1.png](https://d1.awsstatic.com/Products/product-name/diagrams/directory_service_howitworks.80bfccbf2f5d1d63558ec3c086aff247147258f1.png)

    **AWS Directory Service** provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)–aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access.

    **AWS Directory Service Simple AD** is incorrect because this just provides a **subset** of the features offered by AWS Managed Microsoft AD, including the ability to manage user accounts and group memberships, create and apply group policies, securely connect to Amazon EC2 instances, and provide Kerberos-based single sign-on (SSO). In this scenario, the more suitable component to use is the AD Connector since it is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory.

    **IAM Groups** is incorrect because this is just a collection of *IAM* users. *Groups* let you specify permissions for multiple users, which can make it easier to manage the permissions for those users. In this scenario, the more suitable one to use is IAM Roles in order for permissions to create AWS Directory Service resources.

    **Lambda** is incorrect because this is primarily used for serverless computing.

**A company has an On-Demand EC2 instance that is transferring large amounts of data to an Amazon S3 bucket in the same region. Your manager is worried about infrastructure cost considering the vast amounts of data being transferred to the bucket. What will you say to justify this architecture?**

- ​Transferring data from an EC2 instance to an S3 bucket in the same region has no cost at all.
- ​You are only using an On-Demand EC2 instance so the cost will be lower than a Spot instance.
- ​Transferring data from an EC2 instance to an S3 bucket in the same region has a 50% discount based on the AWS Pricing.
- ​You are only using an On-Demand EC2 instance which is exactly the same price as Spot EC2 instance, launched by a persistent Spot request.
- Answer

    **Transferring data from an EC2 instance to Amazon S3, Amazon Glacier, Amazon DynamoDB, Amazon SES, Amazon SQS, or Amazon SimpleDB in the same AWS Region has no cost at all**. Refer to the Amazon EC2 Pricing on the link below for reference.

    The options that say: **You are only using an On-Demand EC2 instance which is exactly the same price as Spot EC2 instance, launched by a persistent Spot request** and **You are only using an On-Demand EC2 instance so the cost will be lower than a Spot instance** are incorrect since an On-Demand instance costs more than a Spot instance.

    The option that says: **Transferring data from an EC2 instance to an S3 bucket in the same region has a 50% discount based on the AWS Pricing** is incorrect as there is no such thing as 50% discount when transferring data from an EC2 instance to an S3 bucket in the same region.

**A financial application is composed of an Auto Scaling group of EC2 instances, an Application Load Balancer, and a MySQL RDS instance in a Multi-AZ Deployments configuration. To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token. As the Solutions Architect of the company, which of the following should you do to meet the above requirement?**

- ​Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance.
- ​Use a combination of IAM and STS to restrict access to your RDS instance via a temporary token.
- ​Enable the IAM DB Authentication.
- ​Configure SSL in your application to encrypt the database connection to RDS.
- Answer

    You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.

    An ***authentication token*** is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-13_07-04-06-a2157247b0fa129795001208504fcb51.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-13_07-04-06-a2157247b0fa129795001208504fcb51.png)

    IAM database authentication provides the following benefits:

    Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).

    You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.

    For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security

    Hence, **enabling IAM DB Authentication** is the correct answer based on the above reference.

    **Configuring SSL in your application to encrypt the database connection to RDS** is incorrect because an SSL connection is not using an authentication token from IAM. Although configuring SSL to your application can improve the security of your data in flight, it is still not a suitable option to use in this scenario.

    **Creating an IAM Role and assigning it to your EC2 instances which will grant exclusive access to your RDS instance** is incorrect because although you can create and assign an IAM Role to your EC2 instances, you still need to configure your RDS to use IAM DB Authentication.

    **Using a combination of IAM and STS to restrict access to your RDS instance via a temporary token** is incorrect because you have to use IAM DB Authentication for this scenario, and not a combination of an IAM and STS. Although STS is used to send temporary tokens for authentication, this is not a compatible use case for RDS.

**A pharmaceutical company has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. In this scenario, which of the following can be used to fulfill this requirement?**

- ​Use IAM users
- ​Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service (AD FS).
- ​Set up SAML 2.0-Based Federation by using a Web Identity Federation.
- ​Use Amazon VPC
- Answer

    Since the company is using Microsoft Active Directory which implements Security Assertion Markup Language (SAML), you can set up a SAML-Based Federation for API Access to your AWS cloud. In this way, you can easily connect to AWS using the login credentials of your on-premises network.

    ![https://docs.aws.amazon.com/IAM/latest/UserGuide/images/saml-based-sso-to-console.diagram.png](https://docs.aws.amazon.com/IAM/latest/UserGuide/images/saml-based-sso-to-console.diagram.png)

    AWS supports identity federation with SAML 2.0, an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS APIs without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code.

    Before you can use SAML 2.0-based federation as described in the preceding scenario and diagram, you must configure your organization's IdP and your AWS account to trust each other. The general process for configuring this trust is described in the following steps. Inside your organization, you must have an IdP that supports SAML 2.0, like Microsoft Active Directory Federation Service (AD FS, part of Windows Server), Shibboleth, or another compatible SAML 2.0 provider.

    Hence, the correct answer is: ***Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service (AD FS)*.**

    ***Setting up SAML 2.0-Based Federation by using a Web Identity Federation*** is incorrect because this is primarily used to let users sign in via a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google. It does not utilize Active Directory.

    ***Using IAM users*** is incorrect because the situation requires you to use the existing credentials stored in their Active Directory, and not user accounts that will be generated by IAM.

    ***Using Amazon VPC*** is incorrect because this only lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. This has nothing to do with user authentication or Active Directory.

**An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability by synchronous data replication to another RDS instance. Which of the following performs synchronous data replication in RDS?**

- ​RDS Read Replica
- ​DynamoDB Read Replica
- ​RDS DB instance running as a Multi-AZ deployment
- ​CloudFront running as a Multi-AZ deployment
- Answer

    When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous **standby** replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-06-07_10-00-40-e7c750751ea701ec7b91cbeeb464f364.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-06-07_10-00-40-e7c750751ea701ec7b91cbeeb464f364.png)

    **RDS Read Replica** is incorrect as a Read Replica provides an asynchronous replication instead of synchronous.

    **DynamoDB Read Replica** and **CloudFront running as a Multi-AZ deployment** are incorrect as both DynamoDB and CloudFront do not have a Read Replica feature.

**You are designing a banking portal which uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands. As the Solutions Architect, which of the following should you do to meet the above requirement?**

- ​Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the `--transit-encryption-enabled` and `--auth-token` parameters enabled.
- ​Set up a Redis replication group and enable the `AtRestEncryptionEnabled`parameter.
- ​Enable the in-transit encryption for Redis replication groups.
- ​Set up an IAM Policy and MFA which requires the Cloud Engineers to enter their IAM credentials and token before they can access the ElastiCache cluster.
- Answer

    Using Redis `AUTH` command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server. Hence, the correct answer is: **Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the `--transit-encryption-enabled` and `--auth-token` parameters enabled.**

    To require that users enter a password on a password-protected Redis server, include the parameter **`--auth-token`** with the correct password when you create your replication group or cluster and on all subsequent commands to the replication group or cluster.

    ![https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/images/ElastiCache-Redis-Secure-Compliant.png](https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/images/ElastiCache-Redis-Secure-Compliant.png)

    **Setting up an IAM Policy and MFA which requires the Cloud Engineers to enter their IAM credentials and token before they can access the ElastiCache cluster** is incorrect because this is not possible in IAM. You have to use the Redis AUTH option instead.

    **Setting up a Redis replication group and enabling the `AtRestEncryptionEnabled`parameter** is incorrect because the Redis At-Rest Encryption feature only secures the data inside the in-memory data store. You have to use Redis AUTH option instead.

    **Enabling the in-transit encryption for Redis replication groups** is incorrect because although in-transit encryption is part of the solution, it is missing the most important thing which is the Redis AUTH option.

**[SAA-C02] A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that is used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.**

```jsx
{
 "Version": "2012-10-17",
 "Statement": [
	 {
		 "Effect": "Allow",
		 "Action": [
			 "s3:Get*",
			 "s3:List*"
			],
			"Resource": "*"
	 },
	 {
		 "Effect": "Allow",
		 "Action": "s3:PutObject",
		 "Resource": "arn:aws:s3:::tutorialsdojo/*"
	 }
	]
}
```

**What does the above IAM policy allow? (Select THREE.)**

- [ ]  ​An IAM user with this IAM policy is allowed to read objects from the `tutorialsdojo` S3 bucket
- [ ]  ​An IAM user with this IAM policy is allowed to read and delete objects from the `tutorialsdojo` S3 bucket.
- [ ]  ​An IAM user with this IAM policy is allowed to change access rights for the `tutorialsdojo` S3 bucket.
- [ ]  ​An IAM user with this IAM policy is allowed to write objects into the `tutorialsdojo` S3 bucket.
- [ ]  ​An IAM user with this IAM policy is allowed to read objects in the `tutorialsdojo` S3 bucket but not allowed to list the objects in the bucket.
- [ ]  ​An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.
- Answer

    You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies.

    IAM policies define permissions for an action regardless of the method that you use to perform the operation. For example, if a policy allows the [GetUser](https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetUser.html) action, then a user with that policy can get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM user, you can choose to allow console or programmatic access. If console access is allowed, the IAM user can sign in to the console using a user name and password. Or if programmatic access is allowed, the user can use access keys to work with the CLI or API.

    ![https://dmhnzl5mp9mj6.cloudfront.net/security_awsblog/images/CC_Diagram1_0717.png](https://dmhnzl5mp9mj6.cloudfront.net/security_awsblog/images/CC_Diagram1_0717.png)

    Based on the provided IAM policy, the user is only allowed to get, write and list all of the objects for the 'tutorialsdojo' s3 bucket. The `s3:PutObject` basically means that you can submit a PUT object request to the S3 bucket to store data.

    Hence, the correct answers are:

    **- *An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.***

    ***- An IAM user with this IAM policy is allowed to write objects into the 'tutorialsdojo' S3 bucket.***

    ***- An IAM user with this IAM policy is allowed to read objects from the 'tutorialsdojo' S3 bucket.***

    The option that says: ***An IAM user with this IAM policy is allowed to change access rights for the 'tutorialsdojo' S3 bucket*** is incorrect because the template does not have any statements which allow the user to change access rights in the bucket.

    The option that says: ***An IAM user with this IAM policy is allowed to read objects in the 'tutorialsdojo' S3 bucket but not allowed to list the objects in the bucket*** is incorrect because it can clearly be seen in the template the there is a `s3:Get*` which permits the user to list objects.

    The option that says: ***An IAM user with this IAM policy is allowed to read and delete objects from the 'tutorialsdojo' S3 bucket*** is incorrect because although you can read objects from the bucket, you cannot delete any objects.

**A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead. In this scenario, how can you protect the backend systems of the platform from traffic spikes?**

- ​Move the Lambda function in a VPC.
- ​Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.
- ​Enable throttling limits and result caching in API Gateway.
- ​Use CloudFront in front of the API Gateway to act as a cache.
- Answer

    Amazon API Gateway provides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any request over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence, **enabling throttling limits and result caching in API Gateway** is the correct answer.

    You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.

    ![https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/09/26/api-gw-settings.png](https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/09/26/api-gw-settings.png)

    The option that says: **Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling** is incorrect since there is no need to transfer your applications to other services.

    **Using CloudFront in front of the API Gateway to act as a cache** is incorrect because CloudFront only speeds up content delivery which provides a better latency experience for your users. It does not help much for the backend.

    **Moving the Lambda function in a VPC** is incorrect because this answer is irrelevant to what is being asked. A VPC is your own virtual private cloud where you can launch AWS services.

**You have triggered the creation of a snapshot of your EBS volume attached to an Instance Store-backed EC2 Instance and is currently on-going. At this point, what are the things that the EBS volume can or cannot do?**

- ​The volume cannot be used until the snapshot completes.
- ​The volume can be used in write-only mode while the snapshot is in progress.
- ​The volume can be used in read-only mode while the snapshot is in progress.
- ​The volume can be used as normal while the snapshot is in progress.
- Answer

    EBS snapshots occur asynchronously which makes the option that says: **The volume can be used as normal while the snapshot is in progress** the correct answer. This means that the point-in-time snapshot is created immediately, but the status of the snapshot is `pending` until the snapshot is complete (when all of the modified blocks have been transferred to Amazon S3), which can take several hours for large initial snapshots or subsequent snapshots where many blocks have changed. While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume hence, you can still use the volume.

    ![https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/snapshot_1a.png](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/snapshot_1a.png)

    The rest of the options are incorrect because you will still be able to perform normal read and write operations on your EBS volume even while a snapshot is ongoing. Although you can take a snapshot of a volume while a previous snapshot of that volume is in the pending status, having multiple pending snapshots of a volume may result in reduced volume performance until the snapshots complete.

**You are using a combination of API Gateway and Lambda for the web services of your online web portal that is being accessed by hundreds of thousands of clients each day. Your company will be announcing a new revolutionary product and it is expected that your web portal will receive a massive number of visitors all around the globe. How can you protect your backend systems and applications from traffic spikes?**

- ​Deploy Multi-AZ in API Gateway with Read Replica
- ​Use throttling limits in API Gateway
- ​Manually upgrade the EC2 instances being used by API Gateway
- ​API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything
- Answer

    **Amazon API Gateway** provides throttling at multiple levels including global and by a service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.

    Amazon API Gateway tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response.

    The option that says: **API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything** is incorrect because although it can scale using AWS Edge locations, you still need to configure the throttling to further manage the bursts of your APIs.

    **Manually upgrading the EC2 instances being used by API Gateway** is incorrect because API Gateway is a fully managed service and hence, you do not have access to its underlying resources.

    **Deploying Multi-AZ in API Gateway with Read Replica** is incorrect because RDS has Multi-AZ and Read Replica capabilities, and not API Gateway.

**The company that you are working for has a highly available architecture consisting of an elastic load balancer and several EC2 instances configured with auto-scaling in three Availability Zones. You want to monitor your EC2 instances based on a particular metric, which is not readily available in CloudWatch. Which of the following is a custom metric in CloudWatch which you have to manually set up?**

- ​Memory Utilization of an EC2 instance
- ​Network packets out of an EC2 instance
- ​Disk Reads activity of an EC2 instance
- ​CPU Utilization of an EC2 instance
- Answer

    CloudWatch has available Amazon EC2 Metrics for you to use for monitoring. **CPU Utilization** identifies the processing power required to run an application upon a selected instance. **Network Utilization** identifies the volume of incoming and outgoing network traffic to a single instance. **Disk Reads/Writes** metric is used to determine the volume of the data the application reads from the hard disk of the instance. This can be used to determine the speed of the application. However, there are certain metrics that are not readily available in CloudWatch such as memory utilization, disk space utilization, and many others which can be collected by setting up a custom metric.

    You need to prepare a custom metric using CloudWatch Monitoring Scripts which is written in Perl. You can also install CloudWatch Agent to collect more system-level metrics from Amazon EC2 instances. Here's the list of custom metrics that you can set up:

    - Memory utilization- Disk swap utilization- Disk space utilization- Page file utilization- Log collection

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-48-03-503799cd694a657201456b3add758b53.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-48-03-503799cd694a657201456b3add758b53.png)

    **CPU Utilization of an EC2 instance**, **Disk Reads activity of an EC2 instance**, and **Network packets out of an EC2 instance** are all incorrect because these metrics are readily available in CloudWatch by default.

**A traffic monitoring and reporting application uses Kinesis to accept real-time data. In order to process and store the data, they used Amazon Kinesis Data Firehose to load the streaming data to various AWS resources. Which of the following services can you load streaming data into?**

- ​Amazon Elasticsearch Service
- ​Amazon S3 Select
- ​Amazon Redshift Spectrum
- ​Amazon Athena
- Answer

    Amazon Kinesis Stream vs Data Firehose vs Analytics

    [SQS, SWF, SNS, API Gateway, Kinesis, Cognito](../SQS%20SWF%20SNS%20API%20Gateway%20Kinesis%20Cognito%201b72e68f3df546f9999c2ce69df0bb5c.md)

    Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, **Amazon Elasticsearch Service**, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today.

    It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.

    ![https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png](https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png)

    **Amazon S3 Select** and **Amazon Redshift Spectrum** are incorrect because Amazon S3 ***Select*** is just a feature of Amazon S3. Likewise, Redshift ***Spectrum*** is also just a feature of Amazon Redshift. Although Amazon Kinesis Data Firehose can load streaming data to both Amazon S3 and Amazon Redshift, it does not directly load the data to S3 Select and Redshift Spectrum.

    S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. Amazon Redshift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required.

    **Amazon Athena** is incorrect because Amazon Kinesis Data Firehose cannot load streaming data to Athena.

**A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system.** 

**Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Select TWO.)**

- [ ]  ​Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.
- [ ]  ​Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.
- [ ]  ​Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.
- [ ]  ​Configure your origin to add a `Cache-Control max-age`directive to your objects, and specify the longest practical value for `max-age` to increase the cache hit ratio of your CloudFront distribution.
- [ ]  ​Use multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. In order to handle the requests faster, set up Lambda functions in each region using the AWS Serverless Application Model (SAM) service.
- Answer

    Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:

    - After CloudFront receives a request from a viewer (viewer request)

    - Before CloudFront forwards the request to the origin (origin request)

    - After CloudFront receives the response from the origin (origin response)

    - Before CloudFront forwards the response to the viewer (viewer response)

    ![https://docs.aws.amazon.com/lambda/latest/dg/images/cloudfront-events-that-trigger-lambda-functions.png](https://docs.aws.amazon.com/lambda/latest/dg/images/cloudfront-events-that-trigger-lambda-functions.png)

    In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing. Therefore, the correct answers are:

    **- Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.**

    **- Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.**

    The option that says: **Use multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. In order to handle the requests faster, set up Lambda functions in each region using the AWS Serverless Application Model (SAM) service** is incorrect because of the same reason provided above. Although setting up multiple VPCs across various regions which are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead.

    The option that says: **Configure your origin to add a `Cache-Control max-age`directive to your objects, and specify the longest practical value for `max-age` to increase the cache hit ratio of your CloudFront distribution** is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the sluggish authentication process of your global users and not just the caching of the static objects.

    The option that says: **Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user** is incorrect because although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with **minimal cost**.

    [https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html)

    ![Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%203.png](Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%203.png)

**An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company. Which S3 encryption technique should the Architect use?**

- ​Use S3 server-side encryption with a KMS managed key.
- ​Use S3 server-side encryption with customer provided key.
- ​Use S3 client-side encryption with a KMS-managed customer master key.
- ​Use S3 client-side encryption with a client-side master key.
- Answer

    **Client-side encryption** is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:

    **- Use an AWS KMS-managed customer master key.**

    **- Use a client-side master key.**

    When using an AWS KMS-managed customer master key to enable client-side data encryption, you provide an AWS KMS customer master key ID (CMK ID) to AWS. **On the other hand, when you use client-side master key for client-side data encryption, your client-side master keys and your unencrypted data are never sent to AWS.** It's important that you safely manage your encryption keys because if you lose them, you can't decrypt your data.

    ![https://media.amazonwebservices.com/blog/2014/s3_sse_customer_key_2.png](https://media.amazonwebservices.com/blog/2014/s3_sse_customer_key_2.png)

    This is how client-side encryption using client-side master key works:

    **When uploading an object** - You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. The process works like this:

    1. The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally. It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object.

    2. The client encrypts the data encryption key using the master key that you provide. The client uploads the encrypted data key and its material description as part of the object metadata. The client uses the material description to determine which client-side master key to use for decryption.

    3. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (`x-amz-meta-x-amz-key`) in Amazon S3.

    **When downloading an object -** The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.

    Hence, the correct answer is to **use S3 client-side encryption with a client-side master key**.

    **Using S3 client-side encryption with a KMS-managed customer master key** is incorrect because in client-side encryption with a KMS-managed customer master key, you provide an AWS KMS customer master key ID (CMK ID) to AWS. The scenario clearly indicates that both the master keys and the unencrypted data should never be sent to AWS.

    **Using S3 server-side encryption with a KMS managed key** is incorrect because the scenario mentioned that the unencrypted data should never be sent to AWS, which means that you have to use client-side encryption in order to encrypt the data first before sending to AWS. In this way, you can ensure that there is no unencrypted data being uploaded to AWS. In addition, the master key used by Server-Side Encryption with AWS KMS–Managed Keys (SSE-KMS) is uploaded and managed by AWS, which directly violates the requirement of not uploading the master key.

    **Using S3 server-side encryption with customer provided key** is incorrect because just as mentioned above, you have to use client-side encryption in this scenario instead of server-side encryption. For the S3 server-side encryption with customer-provided key (SSE-C), you actually provide the encryption key as part of your request to upload the object to S3. Using this key, Amazon S3 manages both the encryption (as it writes to disks) and decryption (when you access your objects).

**There are many clients complaining that the online trading application of an investment bank is always down. Your manager instructed you to re-design the architecture of the application to prevent the unnecessary service interruptions. To ensure high availability, you set up the application to use an ELB to distribute the incoming requests across an auto-scaled group of EC2 instances in two single Availability Zones. The Auto Scaling group is configured with default settings. In this scenario, what happens when an EC2 instance behind an ELB fails a health check?**

- ​The EC2 instance will automatically be deregistered from the default Placement Group.
- ​The EC2 instance gets terminated automatically by the ELB.
- ​The ELB stops sending traffic to the EC2 instance.
- ​The EC2 instance is replaced automatically by the ELB.
- Answer

    In this scenario, the load balancer will route the incoming requests only to the healthy instances. When the load balancer determines that an instance is unhealthy, it stops routing requests to that instance. The load balancer resumes routing requests to the instance when it has been restored to a healthy state.

    There are two ways of checking the status of your EC2 instances:

    1. Via the Auto Scaling group

    2. Via the ELB health checks

    The default health checks for an Auto Scaling group are **EC2 status checks** only. If an instance fails these status checks, the Auto Scaling group considers the instance unhealthy and replaces. If you attached one or more load balancers or target groups to your Auto Scaling group, the group does not, by default, consider an instance unhealthy and replace it if it fails the load balancer health checks.

    However, you can optionally configure the Auto Scaling group to use Elastic Load Balancing health checks. This ensures that the group can determine an instance's health based on additional tests provided by the load balancer. The load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called ***health checks.***

    If you configure the Auto Scaling group to use Elastic Load Balancing health checks, it considers the instance unhealthy if it fails either the EC2 status checks or the load balancer health checks. If you attach multiple load balancers to an Auto Scaling group, all of them must report that the instance is healthy in order for it to consider the instance healthy. If one load balancer reports an instance as unhealthy, **the Auto Scaling group replaces the instance**, even if other load balancers report it as healthy.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2020-03-09_04-31-32-d77e78ac0aadc80040a38821a111bc68.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2020-03-09_04-31-32-d77e78ac0aadc80040a38821a111bc68.png)

    The scenario said that the Auto Scaling group is configured with default settings. This means that it is using the EC2 health check type. Hence, the correct answer is: **The ELB stops sending traffic to the EC2 instance.**

    The option that says: **The EC2 instance gets terminated automatically by the ELB** is incorrect because this action will not be done by ELB.

    The option that says: **The EC2 instance will automatically be deregistered from the default Placement Group** is incorrect because in the first place, an EC2 instance is not associated with a Placement Group by default. A Placement group is simply a logical placement of a group of interdependent EC2 instances to meet the low-latency network performance needs of your workload.

    The option that says: **The EC2 instance is replaced automatically by the ELB** is incorrect because the scenario clearly states that the Auto Scaling group is configured with default settings. The default health check type is the EC2 checks, which means that the ELB will stop sending traffic to the EC2 instance.

**A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for storage of their personal documents.** 

**Which of the following will you need to consider so you can set up a solution that incorporates single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.)**

- [ ]  ​Map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents.
- [ ]  ​Configure an IAM role and an IAM Policy to access the bucket.
- [ ]  ​Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.
- [ ]  ​Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.
- [ ]  ​Set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket.
- Answer

    The question refers to one of the common scenarios for temporary credentials in AWS. Temporary credentials are useful in scenarios that involve identity federation, delegation, cross-account access, and IAM roles. In this example, it is called **enterprise identity federation** considering that you also need to set up a single sign-on (SSO) capability.

    The correct answers are:

    ***- Setup a Federation proxy or an Identity provider***

    ***- Setup an AWS Security Token Service to generate temporary tokens***

    ***- Configure an IAM role and an IAM Policy to access the bucket.***

    ![https://docs.aws.amazon.com/IAM/latest/UserGuide/images/saml-based-federation.diagram.png](https://docs.aws.amazon.com/IAM/latest/UserGuide/images/saml-based-federation.diagram.png)

    In an enterprise identity federation, you can authenticate users in your organization's network, and then provide those users access to AWS without creating new AWS identities for them and requiring them to sign in with a separate user name and password. This is known as the

    *single sign-on*

    (SSO) approach to temporary access. AWS STS supports open standards like Security Assertion Markup Language (SAML) 2.0, with which you can use Microsoft AD FS to leverage your Microsoft Active Directory. You can also use SAML 2.0 to manage your own solution for federating user identities.

    ***Using 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others*** is incorrect since you don't have to use 3rd party solutions to provide the access. AWS already provides the necessary tools that you can use in this situation.

    ***Mapping each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents*** is incorrect as there is no direct way of integrating Amazon S3 with Amazon WorkDocs for this particular scenario. Amazon WorkDocs is simply a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content. And because it’s stored centrally on AWS, you can access it from anywhere on any device.

    ***Setting up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket*** is incorrect since creating that many IAM users would be unnecessary. Also, you want the account to integrate with your AD or LDAP directory, hence, IAM Users does not fit these criteria.

    [https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/](https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/)

**An application hosted in EC2 consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. The Operations team received 5 orders but after a few hours, they saw 20 email notifications in their inbox.** 

**Which of the following could be the possible culprit for this issue?**

- ​The web application is not deleting the messages in the SQS queue after it has processed them.
- ​The web application does not have permission to consume messages in the SQS queue.
- ​The web application is set to short polling so some messages are not being picked up
- ​The web application is set for long polling so the messages are being sent twice.
- Answer

    Always remember that the messages in the SQS queue will continue to exist even after the EC2 instance has processed it, until you delete that message. You have to ensure that you delete the message after processing to prevent the message from being received and processed again once the visibility timeout expires.

    There are three main parts in a distributed messaging system:

    1. The components of your distributed system (EC2 instances)

    2. Your queue (distributed on Amazon SQS servers)

    3. Messages in the queue.

    You can set up a system which has several components that send messages to the queue and receive messages from the queue. The queue redundantly stores the messages across multiple Amazon SQS servers.

    ![https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-message-lifecycle-diagram.png](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-message-lifecycle-diagram.png)

    Refer to the third step of the SQS Message Lifecycle:

    Component 1 sends Message A to a queue, and the message is distributed across the Amazon SQS servers redundantly.

    When Component 2 is ready to process a message, it consumes messages from the queue, and Message A is returned. While Message A is being processed, it remains in the queue and isn't returned to subsequent receive requests for the duration of the visibility timeout.

    Component 2 **deletes** Message A from the queue to prevent the message from being received and processed again once the visibility timeout expires.

    The option that says: **The web application is set for long polling so the messages are being sent twice** is incorrect because long polling helps reduce the cost of using SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). Messages being sent twice in an SQS queue configured with long polling is quite unlikely.

    The option that says: **The web application is set to short polling so some messages are not being picked up** is incorrect since you are receiving emails from SNS where messages are certainly being processed. Following the scenario, messages not being picked up won't result into 20 messages being sent to your inbox.

    The option that says: **The web application does not have permission to consume messages in the SQS queue** is incorrect because not having the correct permissions would have resulted in a different response. The scenario says that messages were properly processed but there were over 20 messages that were sent, hence, there is no problem with the accessing the queue.

**You are a Solutions Architect for a leading Enterprise Resource Planning (ERP) solutions provider and you are instructed to design and set up the architecture of your ERP application in AWS. Your manager instructed you to avoid using fully-managed AWS services and instead, only use specific services which allows you to access the underlying operating system for the resource. This is to allow the company to have a much better control of the underlying resources that their systems are using in the AWS cloud.** 

**Which of the following services should you choose to satisfy this requirement? (Select TWO.)**

- [ ]  ​Amazon Athena
- [ ]  ​Amazon EC2
- [ ]  ​DynamoDB
- [ ]  ​Amazon Neptune
- [ ]  ​Amazon EMR
- Answer

    **Amazon EC2** provides you access to the operating system of the instance that you created.

    **Amazon EMR** provides you a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. You can access the operating system of these EC2 instances that were created by Amazon EMR.

    **Amazon Athena**, **DynamoDB**, and **Amazon Neptune** are incorrect as these are managed services, which means that AWS manages the underlying operating system and other server configurations that these databases use.

**A leading utilities provider is in the process of migrating their applications to AWS. Their Solutions Architect created an EBS-Backed EC2 instance with `ephemeral0`and `ephemeral1` instance store volumes attached to host a web application that fetches and stores data from a web API service.** 

**If this instance is stopped, what will happen to the data on the ephemeral store volumes?**

- ​Data is automatically saved in an EBS volume.
- ​Data is unavailable until the instance is restarted.
- ​Data will be deleted.
- ​Data is automatically saved as an EBS snapshot.
- Answer

    The word ***ephemeral*** means *"short-lived"* or *"temporary"* in the English dictionary. Hence, when you see this word in AWS, always consider this as just a temporary memory or a short-lived storage.

    The virtual devices for instance store volumes are named as `ephemeral[0-23]`. Instance types that support one instance store volume have `ephemeral0`. Instance types that support two instance store volumes have `ephemeral0` and `ephemeral1`, and so on until `ephemeral23`.

    The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost under the following circumstances:

    - The underlying disk drive fails

    - The instance stops

    - The instance terminates

    Hence, the option that says: **Data will be deleted** is the correct answer.

    The option that says: **Data is automatically saved in an EBS volume** is incorrect since **instance store volumes and EBS volumes are two different storage types**. An Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance. An instance store provides temporary block-level storage and is located on disks that are physically attached to the host computer. No automatic backup will be performed.

    The option that says: **Data is unavailable until the instance is restarted** is incorrect because once you stop an instance, the data in the ephemeral instance store volumes will be gone.

    The option that says: **Data is automatically saved as an EBS snapshot** is incorrect because just like in the option above, instance store volumes and EBS volumes are two different storage devices. There is no automated snapshot that will be created.

**In the VPC that you are managing, it has one EC2 instance that has its data stored in an instance store. The instance was shut down by a 2nd level support staff over the weekend to save costs. When you arrived in the office the next Monday, you noticed that all data are lost and are no longer available on the EC2 instance. What might be the cause of this?**

- ​The EC2 instance was using an instance store hence, data will be erased when the instance is stopped or terminated.
- ​The EC2 instance has been hacked.
- ​The EC2 instance was using EBS-backed root volumes hence, the data will be erased when the instance is shut down or stopped.
- ​AWS automatically erased the data due to a virus found on the EC2 instance.
- Answer

    Since you are using an EC2 instance with an Instance store, its data is ephemeral which means that it will be erased once the instance is stopped or terminated. You may argue that the instance was only shut down but remember that the Operating system shutdown commands always terminate an instance store-backed instance. That is why the right answer is the option that says: ***The EC2 instance was using an instance store hence, data will be erased when the instance is stopped or terminated.***

    ![https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png)

    The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost under any of the following circumstances:

    **- The underlying disk drive fails**

    **- The instance stops**

    **- The instance terminates**

    Therefore, do not rely on instance store for valuable, long-term data. Instead, use more durable data storage such as Amazon S3, Amazon EBS, or Amazon EFS. When you stop or terminate an instance, every block of storage in the instance store is reset. Hence, your data cannot be accessed through the instance store of another instance.

    If you create an AMI from an instance, the data on its instance store volumes aren't preserved and aren't present on the instance store volumes of the instances that you launch from the AMI. You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance.

    The option that says: ***The EC2 instance was using EBS-backed root volumes hence, the data will be erased when the instance is shut down or stopped*** is incorrect because the data will persist if you use an EBS-backed root volume.

    The option that says: ***AWS automatically erased the data due to a virus found on the EC2 instance*** is incorrect because based on the AWS Shared Responsibility model, AWS will only manage the underlying resources that the services are using and not your actual data. Hence, it is highly unlikely that AWS will automatically erase your data due to a virus.

    The option that says: ***The EC2 instance has been hacked*** is incorrect because although it is remotely possible that someone got hold of your AWS security credentials and deletes your data, this reason is still far fetched and quite unlikely to happen. Based on the given scenario, the stopping of the instance is one key attribute which we can link to its use of Instance Store volumes.

**You are working as a Solutions Architect in a top software development company in Silicon Valley. The company has multiple applications hosted in their VPC. While you are monitoring the system, you noticed that multiple port scans are coming in from a specific IP address block which are trying to connect to several AWS resources inside your VPC. The internal security team has requested that all offending IP addresses be denied for the next 24 hours for security purposes. Which of the following is the best method to quickly and temporarily deny access from the specified IP addresses?**

- ​Modify the Network Access Control List associated with all public subnets in the VPC to deny access from the IP Address block.
- ​Configure the firewall in the operating system of the EC2 instances to deny access from the IP address block.
- ​Add a rule in the Security Group of the EC2 instances to deny access from the IP Address block.
- ​Create a policy in IAM to deny access from the IP Address block.
- Answer

    To control the traffic coming in and out of your VPC network, you can use the **network access control list (ACL)***.* It is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. This is the best solution among other options as you can easily add and remove the restriction in a matter of minutes.

    **Creating a policy in IAM to deny access from the IP Address block** is incorrect as an IAM policy does not control the inbound and outbound traffic of your VPC.

    **Adding a rule in the Security Group of the EC2 instances to deny access from the IP Address block** is incorrect as although a Security Group acts as a firewall, it will only control both inbound and outbound traffic at the instance level and not on the whole VPC.

    **Configuring the firewall in the operating system of the EC2 instances to deny access from the IP address block** is incorrect because adding a firewall in the underlying operating system of the EC2 instance is not enough; the attacker can just connect to other AWS resources since the network access control list still allows them to do so.

**A web application is using CloudFront to distribute their images, videos, and other static contents stored in their S3 bucket to its users around the world. The company has recently introduced a new member-only access to some of its high quality media files. There is a requirement to provide access to multiple private media files only to their paying subscribers without having to change their current URLs. Which of the following is the most suitable solution that you should implement to satisfy this requirement?**

- ​Create a Signed URL with a custom policy which only allows the members to see the private files.
- ​Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.
- ​Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required `Set-Cookie` headers to the viewer which will unlock the content only to them.
- ​Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.
- Answer

    CloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. If you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following:

    Use **signed URLs** for the following cases:

    - You want to use an RTMP (Real Time Messaging Protocol) distribution. Signed cookies aren't supported for RTMP distributions.

    - You want to restrict access to individual files, for example, an installation download for your application.

    - Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.

    Use **signed cookies** for the following cases:

    - You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of a website.

    - You don't want to change your current URLs.

    Hence, the correct answer for this scenario is the option that says: **Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required `Set-Cookie` headers to the viewer which will unlock the content only to them.**

    The option that says: **Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member** is incorrect because a Match Viewer is an Origin Protocol Policy which configures CloudFront to communicate with your origin using HTTP or HTTPS, depending on the protocol of the viewer request. CloudFront caches the object only once even if viewers make requests using both HTTP and HTTPS protocols.

    The option that says: **Create a Signed URL with a custom policy which only allows the members to see the private files** is incorrect because Signed URLs are primarily used for providing access to individual files, as shown on the above explanation. In addition, the scenario explicitly says that they don't want to change their current URLs which is why implementing Signed Cookies is more suitable than Signed URL.

    The option that says: **Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members** is incorrect because Field-Level Encryption only allows you to securely upload user-submitted sensitive information to your web servers. It does not provide access to download multiple private files.

**There was an incident in your production environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. The issue was escalated to your manager and after a few days, you were instructed to improve the security and protection of your AWS resources. What combination of the following options will protect the S3 objects in your bucket from both accidental deletion and overwriting? (Select TWO.)**

- [ ]  ​Enable Versioning
- [ ]  ​Provide access to S3 data strictly through pre-signed URL only
- [ ]  ​Enable Multi-Factor Authentication Delete
- [ ]  ​Enable Amazon S3 Intelligent-Tiering
- [ ]  ​Disallow S3 Delete using an IAM bucket policy
- Answer

    By using Versioning and enabling MFA (Multi-Factor Authentication) Delete, you can secure and recover your S3 objects from accidental deletion or overwrite.

    Versioning is a means of keeping multiple variants of an object in the same bucket. Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.

    You can also optionally add another layer of security by configuring a bucket to enable MFA (Multi-Factor Authentication) Delete, which requires additional authentication for either of the following operations:

    - Change the versioning state of your bucket

    - Permanently delete an object version

    MFA Delete requires two forms of authentication together:

    - Your security credentials

    - The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device

    **Providing access to S3 data strictly through pre-signed URL only** is incorrect since a pre-signed URL gives access to the object identified in the URL. Pre-signed URLs are useful when customers perform an object upload to your S3 bucket, but does not help in preventing accidental deletes.

    **Disallowing S3 Delete using an IAM bucket policy** is incorrect since you still want users to be able to delete objects in the bucket, and you just want to prevent accidental deletions. Disallowing S3 Delete using an IAM bucket policy will restrict all delete operations to your bucket.

    **Enabling Amazon S3 Intelligent-Tiering** is incorrect since S3 intelligent tiering does not help in this situation.

**You are an AWS Solutions Architect designing an online analytics application that uses Redshift Cluster for its data warehouse. Which service will allow you to monitor all API calls to your Redshift instance and can also provide secured data for auditing and compliance purposes?**

- ​Redshift Spectrum
- ​CloudTrail for security logs
- ​AWS X-Ray
- ​CloudWatch
- Answer

    **AWS CloudTrail** is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.

    CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, API calls, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.

    ![https://media.amazonwebservices.com/blog/2014/cloudtrail_revised_flow_2.png](https://media.amazonwebservices.com/blog/2014/cloudtrail_revised_flow_2.png)

    **CloudWatch** is incorrect because although this is also a monitoring service, it cannot track the API calls to your AWS resources.

    **AWS X-Ray** is incorrect because this is not a suitable service to use to track each API call to your AWS resources. It just helps you debug and analyze your microservices applications with request tracing so you can find the root cause of issues and performance.

    **Redshift Spectrum** is incorrect because this is not a monitoring service but rather a feature of Amazon Redshift that enables you to query and analyze all of your data in Amazon S3 using the open data formats you already use, with no data loading or transformations needed.

**Your cloud architecture is composed of Linux and Windows EC2 instances which process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of your systems, you are required to monitor the memory and disk utilization of all of your instances. Which of the following is the most suitable monitoring solution to implement?**

- ​Use Amazon Inspector and install the Inspector agent to all of your EC2 instances.
- ​Enable the Enhanced Monitoring option in EC2 and install CloudWatch agent to all of your EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.
- ​Use the default CloudWatch configuration to your EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all of your EC2 instances.
- ​Install the CloudWatch agent to all of your EC2 instances which gathers the memory and disk utilization data. View the custom metrics in the Amazon CloudWatch console.
- Answer

    CloudWatch has available Amazon EC2 Metrics for you to use for monitoring 
    **CPU utilization, Network utilization, Disk performance, and Disk Reads/Writes**. 
    In case that you need to monitor the below items, you need to prepare a custom metric using a Perl or other shell script, as there are no ready to use metrics for these:

    Memory utilization

    disk swap utilization

    disk space utilization

    page file utilization

    log collection

    Take note that there is a multi-platform CloudWatch agent which can be installed on both Linux and Windows-based instances. You can use a single agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. This agent supports both Windows Server and Linux and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. It is recommended that you use the new agent instead of the older monitoring scripts to collect metrics and logs.

    ![https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-container-cw.png](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-container-cw.png)

    The option that says: **Use the default CloudWatch configuration to your EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all of your EC2 instances** is incorrect because, by default, CloudWatch does not automatically provide memory and disk utilization metrics of your instances. You have to set up custom CloudWatch metrics to monitor the memory, disk swap, disk space and page file utilization of your instances.

    The option that says: **Enable the Enhanced Monitoring option in EC2 and install CloudWatch agent to all of your EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard** is incorrect because Enhanced Monitoring is a feature of RDS and not of CloudWatch.

    **Using Amazon Inspector and installing the Inspector agent to all of your EC2 instances** is incorrect because Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. It does not provide a custom metric to track the memory and disk utilization of each and every EC2 instance in your VPC.

**You are building a new data analytics application in AWS which will be deployed in an Auto Scaling group of On-Demand EC2 instances and a MongoDB database. It is expected that the database will have high-throughput workloads performing small, random I/O operations. As the Solutions Architect, you are required to properly set up and launch the required resources in AWS. Which of the following is the most suitable EBS type to use for your database?**

- ​Provisioned IOPS SSD (io1)
- ​General Purpose SSD (gp2)
- ​Cold HDD (sc1)
- ​Throughput Optimized HDD (st1)
- Answer

    On a given volume configuration, certain I/O characteristics drive the performance behavior for your EBS volumes. SSD-backed volumes, such as General Purpose SSD (`gp2`) and Provisioned IOPS SSD (`io1`), deliver consistent performance whether an I/O operation is random or sequential. HDD-backed volumes like Throughput Optimized HDD (`st1`) and Cold HDD (`sc1`) deliver optimal performance only when I/O operations are large and sequential.

    In the exam, always consider the difference between SSD and HDD as shown on the table below. This will allow you to easily eliminate specific EBS-types in the options which are not SSD or not HDD, depending on whether the question asks for a storage type which has ***small, random*** I/O operations or ***large, sequential*** I/O operations.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png)

    Provisioned IOPS SSD (`io1`) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike `gp2`, which uses a bucket and credit model to calculate performance, an `io1` volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_05-52-53-2f6fe79f1600d1f1c4eaab87872423d7.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_05-52-53-2f6fe79f1600d1f1c4eaab87872423d7.png)

    **General Purpose SSD (gp2)** is incorrect because although General Purpose is a type of SSD that can handle small, random I/O operations, the Provisioned IOPS SSD volumes are much more suitable to meet the needs of I/O-intensive database workloads such as MongoDB, Oracle, MySQL, and many others.

    **Throughput Optimized HDD (st1)** and **Cold HDD (sc1)** are incorrect because HDD volumes (such as Throughput Optimized HDD and Cold HDD volumes) are more suitable for workloads with large, sequential I/O operations instead of small, random I/O operations.

**An application is hosted in an AWS Fargate cluster that runs a batch job whenever an object is loaded on an Amazon S3 bucket. The minimum number of ECS Tasks is initially set to 1 to save on costs, and it will only increase the task count based on the new objects uploaded on the S3 bucket. Once processing is done, the bucket becomes empty and the ECS Task count should be back to 1.Which is the most suitable option to implement with the LEAST amount of effort?**

- ​Set up an alarm in CloudWatch to monitor CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event.
- ​Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to a Lambda function that will run Amazon ECS API command to increase the number of tasks on ECS. Create another rule to detect S3 DELETE operations and run the Lambda function to reduce the number of ECS tasks.
- ​Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to the ECS cluster with the increased number of tasks. Create another rule to detect S3 DELETE operations and set the target to the ECS Cluster with 1 as the Task count.
- ​Set up an alarm in CloudWatch to monitor CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event.
- Answer

    You can use CloudWatch Events to run Amazon ECS tasks when certain AWS events occur. You can set up a CloudWatch Events rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using the Amazon S3 PUT operation. You can also declare a reduced number of ECS tasks whenever a file is deleted on the S3 bucket using the DELETE operation.

    First, you must create a CloudWatch Events rule for the S3 service that will watch for object-level operations – PUT and DELETE objects. For object-level operations, it is required to create a CloudTrail trail first. On the Targets section, select the “ECS task” and input the needed values such as the cluster name, task definition and the task count. You need two rules – one for the scale-up and another for the scale-down of the ECS task count.

    ![https://docs.aws.amazon.com/AmazonECS/latest/userguide/images/overview-fargate.png](https://docs.aws.amazon.com/AmazonECS/latest/userguide/images/overview-fargate.png)

    Hence, the correct answer is: ***Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to the ECS cluster with the increased number of tasks. Create another rule to detect S3 DELETE operations and set the target to the ECS Cluster with 1 as the Task count.***

    The option that says: ***Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to a Lambda function that will run Amazon ECS API command to increase the number of tasks on ECS. Create another rule to detect S3 DELETE operations and run the Lambda function to reduce the number of ECS tasks*** is incorrect because although this solution meets the requirement, creating your own Lambda function for this scenario is not really necessary. It is much simpler to control ECS task directly as target for the CloudWatch Event rule. Take note that the scenario asks for a solution that is the easiest to implement.

    The option that says: ***Set up an alarm in CloudWatch to monitor CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event*** is incorrect because using CloudTrail, CloudWatch Alarm, and two Lambda functions creates an unnecessary complexity to what you want to achieve. CloudWatch Events can directly target an ECS task on the Targets section when you create a new rule.

    The option that says: ***Set up an alarm in CloudWatch to monitor CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event*** is incorrect because you can’t directly set CloudWatch Alarms to update the ECS task count. You have to use CloudWatch Events instead.

**You have a web application deployed in AWS which is currently running in the eu-central-1 region. You have an Auto Scaling group of On-Demand EC2 instances which are using pre-built AMIs. Your manager instructed you to implement disaster recovery for your system so in the event that the application goes down in the eu-central-1 region, a new instance can be started in the us-west-2 region. As part of your disaster recovery plan, which of the following should you take into consideration?**

- ​In the AMI dashboard, add the us-west-2 region to the Network Access Control List which contains the regions that are allowed to use the AMI.
- ​Copy the AMI from the eu-central-1 region to the us-west-2 region. Afterwards, create a new Auto Scaling group in the us-west-2 region to use this new AMI ID.
- ​None. AMIs can be used in any region hence, there is no problem using it in the us-west-2 region.
- ​Share the AMI to the us-west-2 region.
- Answer

    In this scenario, the EC2 instances you are currently using depends on a pre-built AMI. This AMI is not accessible to another region hence, you have to copy it to the us-west-2 region to properly establish your disaster recovery instance.

    You can copy an Amazon Machine Image (AMI) within or across an AWS region using the AWS Management Console, the AWS command line tools or SDKs, or the Amazon EC2 API, all of which support the `CopyImage` action. You can copy both Amazon EBS-backed AMIs and instance store-backed AMIs. You can copy encrypted AMIs and AMIs with encrypted snapshots.

    The options that say: **In the AMI dashboard, add the us-west-2 region to the Network Access Control List which contains the regions that are allowed to use the AMI** and **Share the AMI to the us-west-2 region** are incorrect because the AMI does not have a Network Access Control nor a Share functionality.

    The option that says: **None. AMIs can be used in any region hence, there is no problem using it in the us-west-2 region** is incorrect as you can use a unique or pre-built AMI to a specific region only.

**You are working for a large financial company as an IT consultant. Your role is to help their development team to build a highly available web application using stateless web servers. In this scenario, which AWS services are suitable for storing session state data? (Select TWO.)**

- [ ]  ​Glacier
- [ ]  ​DynamoDB
- [ ]  ​RDS
- [ ]  ​ElastiCache
- [ ]  ​Redshift Spectrum
- Answer

    **DynamoDB** and **ElastiCache** are the correct answers. You can store session state data on both DynamoDB and ElastiCache. These AWS services provide high-performance storage of key-value pairs which can be used to build a highly available web application.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-00-29-3a01a791766edb97563653a3c08ac334.PNG](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-00-29-3a01a791766edb97563653a3c08ac334.PNG)

    **Redshift Spectrum** is incorrect since this is a data warehousing solution where you can directly query data from your data warehouse. Redshift is not suitable for storing session state, but more on analytics and OLAP processes.

    **RDS** is incorrect as well since this is a relational database solution of AWS. This relational storage type might not be the best fit for session states, and it might not provide the performance you need compared to DynamoDB for the same cost.

    **S3 Glacier** is incorrect as well since this is a low-cost cloud storage service for data archiving and long-term backup. The archival and retrieval speeds of Glacier is too slow for handling session states.

**You have a requirement to make sure that an On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?**

- ​Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/32
- ​Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/0
- ​Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32
- ​Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/0
- Answer

    The SSH protocol uses TCP and port 22. Hence, **Protocol – UDP, Port Range – 22, Source 110.238.98.71/32** and **Protocol – UDP, Port Range – 22, Source 110.238.98.71/0** are incorrect as they are using UDP.

    The following two options: **Protocol – TCP, Port Range – 22, Source 110.238.98.71/32**and **Protocol – TCP, Port Range – 22, Source 110.238.98.71/0** have one major difference and that is their CIDR block.

    The requirement is to only allow the individual IP of the client and not the entire network. Therefore, the proper CIDR notation should be used. The **/32** denotes one IP address and the **/0** refers to the entire network. That is why **Protocol – TCP, Port Range – 22, Source 110.238.98.71/0** is incorrect as it allowed the entire network instead of a single IP.

**A Solutions Architect is working for a company which has multiple VPCs in various AWS regions. The Architect is assigned to set up a logging system which will track all of the changes made to their AWS resources in all regions, including the configurations made in IAM, CloudFront, AWS WAF, and Route 53. In order to pass the compliance requirements, the solution must ensure the security, integrity, and durability of the log data. It should also provide an event history of all API calls made in AWS Management Console and AWS CLI. Which of the following solutions is the best fit for this scenario?**

- ​Set up a new CloudWatch trail in a new S3 bucket using the CloudTrail console and also pass the `--is-multi-region-trail` parameter then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.
- ​Set up a new CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the `--is-multi-region-trail` and `--include-global-service-events` parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.
- ​Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the `--is-multi-region-trail` and `--include-global-service-events` parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies
- ​Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the `--is-multi-region-trail` and `--no-include-global-service-events` parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.
- Answer

    An event in CloudTrail is the record of an activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. There are two types of events that can be logged in CloudTrail: management events and data events. By default, trails log management events, but not data events.

    ![https://media.amazonwebservices.com/blog/2015/cloudtrail_all_regions_main_screen_turn_on_1.png](https://media.amazonwebservices.com/blog/2015/cloudtrail_all_regions_main_screen_turn_on_1.png)

    A trail can be applied to all regions or a single region. As a best practice, create a trail that applies to all regions in the AWS partition in which you are working. This is the default setting when you create a trail in the CloudTrail console.

    For most services, events are recorded in the region where the action occurred. For global services such as AWS Identity and Access Management (IAM), AWS STS, Amazon CloudFront, and Route 53, events are delivered to any trail that includes global services, and are logged as occurring in US East (N. Virginia) Region.

    In this scenario, the company requires a secure and durable logging solution that will track all of the activities of all AWS resources on all regions. CloudTrail can be used for this case with multi-region trail enabled, however, it will only cover the activities of the regional services (EC2, S3, RDS etc.) and not for global services such as IAM, CloudFront, AWS WAF, and Route 53. In order to satisfy the requirement, you have to add the `--include-global-service-events` parameter in your AWS CLI command.

    The option that says: ***Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the `--is-multi-region-trail` and `--include-global-service-events` parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies*** is correct because it provides security, integrity, and durability to your log data and in addition, it has the -include-global-service-events parameter enabled which will also include activity from global services such as IAM, Route 53, AWS WAF, and CloudFront.

    The option that says: ***Set up a new CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the `--is-multi-region-trail` and `--include-global-service-events` parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies*** is incorrect because you need to use CloudTrail instead of CloudWatch.

    The option that says: ***Set up a new CloudWatch trail in a new S3 bucket using the CloudTrail console and also pass the `--is-multi-region-trail` parameter then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies*** is incorrect because you need to use CloudTrail instead of CloudWatch. In addition, the --include-global-service-events parameter is also missing in this setup.

    The option that says: ***Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the `--is-multi-region-trail` and `--no-include-global-service-events` parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies*** is incorrect because the `--is-multi-region-trail` is not enough as you also need to add the `--include-global-service-events` parameter and not `--no-include-global-service-events`. Plus, you cannot enable the Global Service Events using the CloudTrail console but by using AWS CLI.

**A multi-tiered application hosted in your on-premises data center is scheduled to be migrated to AWS. The application has a message broker service which uses industry standard messaging APIs and protocols that must be migrated as well, without rewriting the messaging code in your application. Which of the following is the most suitable service that you should use to move your messaging service to AWS?**

- ​Amazon SQS
- ​Amazon SWF
- ​Amazon SNS
- ​Amazon MQ
- Answer

    Amazon MQ, Amazon SQS, and Amazon SNS are messaging services that are suitable for anyone from startups to enterprises. If you're using messaging with existing applications and want to move your messaging service to the cloud quickly and easily, it is recommended that you consider Amazon MQ. **It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications.**

    Hence, **Amazon MQ** is the correct answer.

    ![https://media.amazonwebservices.com/blog/2017/mq_con_4.png](https://media.amazonwebservices.com/blog/2017/mq_con_4.png)

    **If you are building brand new applications in the cloud, then it is highly recommended that you consider Amazon SQS and Amazon SNS**. Amazon SQS and SNS are lightweight, fully managed message queue and topic services that scale almost infinitely and provide simple, easy-to-use APIs. You can use Amazon SQS and SNS to decouple and scale microservices, distributed systems, and serverless applications, and improve reliability.

    **Amazon SQS** is incorrect because although this is a fully managed message queuing service, it does not support an extensive list of industry-standard messaging APIs and protocol, unlike Amazon MQ. Moreover, using Amazon SQS requires you to do additional changes in the messaging code of applications to make it compatible.

    **Amazon SNS** is incorrect because SNS is more suitable as a pub/sub messaging service instead of a message broker service.

    **Amazon SWF** is incorrect because this is a fully-managed state tracker and task coordinator service and not a messaging service, unlike Amazon MQ, AmazonSQS and Amazon SNS.

**You are working as a Solutions Architect in a new startup that provides storage for high-quality photos which are infrequently accessed by the users. To make the architecture cost-effective, you designed the cloud service to use an S3 One Zone-Infrequent Access (S3 One Zone-IA) storage type for free users and an S3 Standard-Infrequent Access (S3 Standard-IA) storage type for premium users. When your manager found out about this, he asked you about the differences between using S3 One Zone-IA and S3 Standard-IA.** 

**What will you say to your manager? (Select TWO.)**

- [ ]  ​S3 One Zone-IA offers lower durability and low throughput compared with Amazon S3 Standard and S3 Standard-IA which is why it has a low per GB storage price and per GB retrieval fee.
- [ ]  ​Storing data in S3 One Zone-IA costs less than storing it in S3 Standard-IA.
- [ ]  ​Unlike other Amazon object storage classes, which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ.
- [ ]  ​Storing data in S3 One Zone-IA costs more than storing it in S3 Standard-IA but provides more durability.
- [ ]  ​Unlike other Amazon object storage classes, which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in two AZs only. Hence the name, One Zone-IA since the data replication is skipped in one Availability Zone.
- Answer

    Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) is an Amazon S3 storage class for data that is accessed less frequently but requires rapid access when needed. Unlike other Amazon object storage classes, which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ. Because of this, storing data in S3 One Zone-IA costs 20% less than storing it in S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA storage. It’s a good choice, for example, for storing secondary backup copies of on-premises data or easily re-creatable data, or for storage used as an S3 Cross-Region Replication target from another AWS Region.

    S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard and S3 Standard-IA, with a low per GB storage price and per GB retrieval fee. The S3 One Zone-IA storage class is set at the object level and can exist in the same bucket as S3 Standard and S3 Standard-IA, allowing you to use S3 Lifecycle Policies to automatically transition objects between storage classes without any application changes.

    **Key Features:**

    Same low latency and high throughput performance of S3 Standard and S3 Standard-IA

    Designed for durability of 99.999999999% of objects in a single Availability Zone, but data will be lost in the event of Availability Zone destruction

    Designed for 99.5% availability over a given year

    Backed with the Amazon S3 Service Level Agreement for availability

    Supports SSL for data in transit and encryption of data at rest

    Lifecycle management for automatic migration of objects

    Remember that since the S3 One Zone-IA stores data in a single AWS Availability Zone, data stored in this storage class will be lost in the event of Availability Zone destruction.

**A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future. Which of the following is the most suitable solution to meet the requirement?**

- ​Create an Oracle database in RDS with Multi-AZ deployments.
- ​Launch an Oracle database instance in RDS with Recovery Manager (RMAN) enabled.
- ​Launch an Oracle Real Application Clusters (RAC) in RDS.
- ​Migrate your Oracle data to Amazon Aurora by converting the database schema using AWS Schema Conversion Tool and AWS Database Migration Service.
- Answer

    Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.

    ![https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png)

    In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.

    In this scenario, the best RDS configuration to use is an Oracle database in RDS with Multi-AZ deployments to ensure high availability even if the primary database instance goes down. Hence, **creating an Oracle database in RDS with Multi-AZ deployments** is the correct answer.

    **Launching an Oracle database instance in RDS with Recovery Manager (RMAN) enabled** and **launching an Oracle Real Application Clusters (RAC) in RDS** are incorrect because Oracle RMAN and RAC are not supported in RDS.

    **Migrating your Oracle data to Amazon Aurora by converting the database schema using AWS Schema Conversion Tool and AWS Database Migration Service** is incorrect because although this solution is feasible, it takes time to migrate your Oracle database to Aurora which is not acceptable. Based on this option, the Aurora database does not have a Read Replica and is not configured as an Amazon Aurora DB cluster, which could have improved the availability of the database.

**A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances which use Amazon Aurora as its database. Currently, the system stores the file documents that the users uploaded in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system. In this scenario, what will you do to implement a scalable, high throughput POSIX-compliant file system?**

- ​Create an S3 bucket and use this as the storage for the CMS
- ​Use EFS
- ​Upgrade your existing EBS volumes to Provisioned IOPS SSD Volumes
- ​Use ElastiCache
- Answer

    **Amazon Elastic File System (Amazon EFS)** provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. When mounted on Amazon EC2 instances, an Amazon EFS file system provides a standard file system interface and file system access semantics, allowing you to seamlessly integrate Amazon EFS with your existing applications and tools. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, allowing Amazon EFS to provide a common data source for workloads and applications running on more than one Amazon EC2 instance.

    This particular scenario tests your understanding of EBS, EFS, and S3. In this scenario, there is a fleet of On-Demand EC2 instances that stores file documents from the users to one of the attached EBS Volumes. The system performance is quite slow because the architecture doesn't provide the EC2 instances a parallel shared access to the file documents.

    Remember that an EBS Volume can be attached to one EC2 instance at a time, hence, no other EC2 instance can connect to that EBS Provisioned IOPS Volume. Take note as well that the type of storage needed here is a "file storage" which means that **S3** is not the best service to use because it is mainly used for "object storage", and S3 does not provide the notion of "folders" too. This is why **using EFS** is the correct answer.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-55-39-826ea8db5a1179c5f79d02b3b6cf1f2c.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-55-39-826ea8db5a1179c5f79d02b3b6cf1f2c.png)

    **Upgrading your existing EBS volumes to Provisioned IOPS SSD Volumes** is incorrect because the scenario requires you to set up a scalable, high throughput storage system that will allow concurrent access from multiple EC2 instances. This is clearly not possible in EBS, even with Provisioned IOPS SSD Volumes. You have to use EFS instead.

    **Using ElastiCache** is incorrect because this is an in-memory data store that improves the performance of your applications, which is not what you need since it is not a file storage.

**Your company announced that there would be a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Scheduled Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance.** 

**Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Select TWO.)**

- [ ]  ​Reserved Instances doesn't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand.
- [ ]  ​You can have capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term through Scheduled Reserved Instances
- [ ]  ​Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances
- [ ]  ​It runs in a VPC on hardware that's dedicated to a single customer.
- [ ]  ​It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration.
- Answer

    Reserved Instances (RIs) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. You have the flexibility to change families, OS types, and tenancies while benefiting from RI pricing when you use Convertible RIs. One important thing to remember here is that Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account.

    When your computing needs change, you can modify your Standard or Convertible Reserved Instances and continue to take advantage of the billing benefit. You can modify the Availability Zone, scope, network platform, or instance size (within the same instance type) of your Reserved Instance. You can also sell your unused instance on the Reserved Instance Marketplace.

    The option that says: **Reserved Instances don't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand** is correct. Likewise, the option that says: **You can have capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term through Scheduled Reserved Instances** is correct. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them.

    The option that says: **Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances** is incorrect because only Convertible Reserved Instances can be exchanged for other Convertible Reserved Instances.

    The option that says: **It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration**is incorrect because you can reserve capacity to a specific AWS Region (regional Reserved Instance) or specific Availability Zone (zonal Reserved Instance) only. You cannot reserve capacity to multiple AWS Regions in a single RI purchase.

    The option that says: **It runs in a VPC on hardware that's dedicated to a single customer** is incorrect because that is the description of a Dedicated instance and not a Reserved Instance. A Dedicated instance runs in a VPC on hardware that's dedicated to a single customer.

**A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS. They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges. The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. Which of the following is the best and most cost-effective approach to implement in this scenario?**

- ​Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.
- ​Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an S3 bucket, and then later to Glacier for archival.
- ​Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.
- ​Use AWS Snowmobile to migrate all of the files from the on-premises network. Upload the documents to an S3 bucket and set up a lifecycle policy to move the data into Glacier for archival.
- Answer

    A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.

    ![https://docs.aws.amazon.com/storagegateway/latest/userguide/images/file-gateway-concepts-diagram.png](https://docs.aws.amazon.com/storagegateway/latest/userguide/images/file-gateway-concepts-diagram.png)

    The gateway provides access to objects in S3 as files or file share mount points. With a file gateway, you can do the following:

    - You can store and retrieve files directly using the NFS version 3 or 4.1 protocol.

    - You can store and retrieve files directly using the SMB file system version, 2 and 3 protocol.

    - You can access your data directly in Amazon S3 from any AWS Cloud application or service.

    - You can manage your Amazon S3 data using lifecycle policies, cross-region replication, and versioning. You can think of a file gateway as a file system mount on S3.

    AWS Storage Gateway supports the Amazon S3 Standard, Amazon S3 Standard-Infrequent Access, Amazon S3 One Zone-Infrequent Access and Amazon Glacier storage classes. When you create or update a file share, you have the option to select a storage class for your objects. You can either choose the Amazon S3 Standard or any of the infrequent access storage classes such as S3 Standard IA or S3 One Zone IA. Objects stored in any of these storage classes can be transitioned to Amazon Glacier using a Lifecycle Policy.

    Although you can write objects directly from a file share to the S3-Standard-IA or S3-One Zone-IA storage class, it is recommended that you use a Lifecycle Policy to transition your objects rather than write directly from the file share, especially if you're expecting to update or delete the object within 30 days of archiving it.

    Therefore, the correct answer is: ***Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.***

    The option that says: ***Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival*** is incorrect because although tape gateways provide cost-effective and durable archive backup data in Amazon Glacier, it does not meet the criteria of being retrievable immediately within minutes. It also doesn't maintain a local cache that provides low latency access to the recently accessed data and reduce data egress charges. Thus, it is still better to set up a file gateway instead.

    The option that says: ***Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an S3 bucket, and then later to Glacier for archival*** is incorrect because EBS Volumes are not as durable compared with S3 and it would be more cost-efficient if you directly store the documents to an S3 bucket. An alternative solution is to use AWS Direct Connect with AWS Storage Gateway to create a connection for high-throughput workload needs, providing a dedicated network connection between your on-premises file gateway and AWS. But this solution is using EBS, hence, this option is still wrong.

    The option that says: ***Use AWS Snowmobile to migrate all of the files from the on-premises network. Upload the documents to an S3 bucket and set up a lifecycle policy to move the data into Glacier for archival*** is incorrect because Snowmobile is mainly used to migrate the entire data of an on-premises data center to AWS. This is not a suitable approach as the company still has a hybrid cloud architecture which means that they will still use their on-premises data center along with their AWS cloud infrastructure.

Attempts

1. 55%

![Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%204.png](Test%201%2013662dc24b254c8aa8ac44896c383326/Untitled%204.png)