# Test 6

Moving and attaching the EBS volume to a new EC2 instance in another AZ:
- Create a snapshot then create a volume using the snapshot in the other AZ
~~- Detach EBS volume and attach it to an EC2 instance residing in another AZ~~
EBS volume is only available in the Availability Zone it was created in and cannot be attached directly to other Availability Zones.

Set up multi-account AWS environment?
AWS Organizations or IAM
Setting up a common IAM policy that can be applied across all AWS accounts is incorrect because it is not possible to create a common IAM policy for multiple AWS accounts.

Aurora prevent any downtime?
This is for performance or DR? 
Performance is Aurora Replicas
To prevent downtime? 
Use Auto-Scaling across multiple AZ with ELB is incorrect because Aurora is a database engine for RDS and not deployed on a typical EC2 instance.

Multicast networking?

Connect on-premises infra to AWS Cloud
Direct Connect, IPsec VPN connection, Amazon Connect

Route table with rule 10.0.0.0/27 target local?

Update resources fleet of EC2 instances inside an Auto Scaling Group?
CloudFormation
CodeBuild - CicleCI like
CodeCommit - Github like
CodeDeploy - for deploy

Cluster placement group
Add new instances to the placement group that already has running EC2 instances
'insufficient capacity error'

Deploy a HPC cluster, workload processing, data accessed via a fast and scalable file system interface, work natively with S3, process S3 with high-performance POSIX interface
Amazon FSx for Lustre

Latency based routing
ALB Route53 NetworkLB?
Load balancers distribute traffic only within their respective regions and not to other AWS regions.

Run task every night at 10 PM to 3 AM
Use Scheduled Reserved Instance
On-Demand EC2 instances? incorrect because although an On-Demand instance is stable and suitable for processing critical data, it costs more than any other option

Kinesis Data Stream vs Data Firehose?

AWS host public datasets such as satelite imagery geospatial or genomic data
Use this dataset, how much it cost?

OpsWorks
CodeDeploy

**A global medical research company has a molecular imaging system which provides each client with frequently updated images of what is happening inside the human body at the molecular and cellular level. The system is hosted in AWS and the images are hosted in an S3 bucket behind a CloudFront web distribution. There was a new batch of updated images that were uploaded in S3, however, the users were reporting that they were still seeing the old content. You need to control which image will be returned by the system even when the user has another version cached either locally or behind a corporate caching proxy. Which of the following is the most suitable solution to solve this issue?**

- ​Add a separate cache behavior path for the content and configure a custom object caching with a Minimum TTL of 0
- ​Use versioned objects
- ​Invalidate the files in your CloudFront web distribution
- ​Add Cache-Control no-cache, no-store, or private directives in the S3 bucket

### **Explanation**

- Answer

    To control the versions of files that are served from your distribution, you can either invalidate files or give them versioned file names. If you want to update your files frequently, AWS recommends that you primarily use file versioning for the following reasons:

    - Versioning enables you to control which file a request returns even when the user has a version cached either locally or behind a corporate caching proxy. **If you invalidate the file, the user might continue to see the old version until it expires from those caches.**

    - CloudFront access logs include the names of your files, so versioning makes it easier to analyze the results of file changes.

    - Versioning provides a way to serve different versions of files to different users.

    - Versioning simplifies rolling forward and back between file revisions.

    - Versioning is less expensive. You still have to pay for CloudFront to transfer new versions of your files to edge locations, but you don't have to pay for invalidating files.

    **Invalidating the files in your CloudFront web distribution** is incorrect because even though using invalidation will solve this issue, this solution is more expensive as compared to **using versioned objects**.

    **Adding a separate cache behavior path for the content and configuring a custom object caching with a Minimum TTL of 0** is incorrect because this alone is not enough to solve the problem. A cache behavior is primarily used to configure a variety of CloudFront functionality for a given URL path pattern for files on your website. Although this solution may work, it is still better to use versioned objects where you can control which image will be returned by the system even when the user has another version cached either locally or behind a corporate caching proxy.

    **Adding Cache-Control no-cache, no-store, or private directives in the S3 bucket** is incorrect because although it is right to configure your origin to add the ***Cache-Control*** or ***Expires*** header field, you should do this to your objects and not on the entire S3 bucket.

**A commercial bank has designed their next generation online banking platform to use a distributed system architecture. As their Software Architect, you have to ensure that their architecture is highly scalable, yet still cost-effective. Which of the following will provide the most suitable solution for this scenario?**

- ​Launch multiple EC2 instances behind an Application Load Balancer to host your application services, and SWF which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.
- ​Launch an Auto-Scaling group of EC2 instances to host your application services and an SQS queue. Include an Auto Scaling trigger to watch the SQS queue size which will either scale in or scale out the number of EC2 instances based on the queue.
- ​Launch multiple On-Demand EC2 instances to host your application services and an SQS queue which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.
- ​Launch multiple EC2 instances behind an Application Load Balancer to host your application services and SNS which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.

### **Explanation**

- Answer

    There are three main parts in a distributed messaging system: the components of your distributed system which can be hosted on EC2 instance; your queue (distributed on Amazon SQS servers); and the messages in the queue.

    To improve the scalability of your distributed system, you can add Auto Scaling group to your EC2 instances.

**A startup company wants to launch a fleet of EC2 instances on AWS. Your manager wants to ensure that the Java programming language is installed automatically when the instance is launched. In which of the below configurations can you achieve this requirement?**

- ​AWS Config
- ​EC2Config service
- ​IAM roles
- ​User data

### **Explanation**

- Answer

    When you launch an instance in Amazon EC2, you have the option of passing **user data** to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can write and run scripts that install new packages, software, or tools in your instance when it is launched.

    You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls).

**A Solutions Architect is developing a three-tier cryptocurrency web application for a FinTech startup. The Architect has been instructed to restrict access to the database tier to only accept traffic from the application-tier and deny traffic from other sources. The application-tier is composed of application servers hosted in an Auto Scaling group of EC2 instances.Which of the following options is the MOST suitable solution to implement in this scenario?**

- ​Set up the security group of the database tier to allow database traffic from a specified list of application server IP addresses.
- ​Set up the security group of the database tier to allow database traffic from the security group of the application servers.
- ​Set up the Network ACL of the database subnet to deny all inbound non-database traffic from the subnet of the application-tier.
- ​Set up the Network ACL of the database subnet to allow inbound database traffic from the subnet of the application-tier.

### **Explanation**

- Answer

    A **security group** acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups. If you don't specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.

    ![https://docs.aws.amazon.com/vpc/latest/userguide/images/security-diagram.png](https://docs.aws.amazon.com/vpc/latest/userguide/images/security-diagram.png)

    For each security group, you add *rules* that control the inbound traffic to instances, and a separate set of rules that control the outbound traffic. This section describes the basic things you need to know about security groups for your VPC and their rules.

    You can add or remove rules for a security group which is also referred to as *authorizing* or *revoking* inbound or outbound access. A rule applies either to inbound traffic (ingress) or outbound traffic (egress). You can grant access to a specific CIDR range, **or to another security group** in your VPC or in a peer VPC (requires a VPC peering connection).

    In the scenario, the servers of the application-tier are in an Auto Scaling group which means that the number of EC2 instances could grow or shrink over time. An Auto Scaling group could also cover one or more Availability Zones (AZ) which have their own subnets. Hence, the most suitable solution would be to **set up the security group of the database tier to allow database traffic from the security group of the application servers** since you can utilize the security group of the application-tier Auto Scaling group as the source for the security group rule in your database tier.

    **Setting up the security group of the database tier to allow database traffic from a specified list of application server IP addresses** is incorrect because the list of application server IP addresses will change over time since an Auto Scaling group can add or remove EC2 instances based on the configured scaling policy. This will create inconsistencies in your application because the newly launched instances, which are not included in the initial list of IP addresses, will not be able to access the database.

    **Setting up the Network ACL of the database subnet to deny all inbound non-database traffic from the subnet of the application-tier** is incorrect because doing this could affect the other EC2 instances of other applications, which are also hosted in the same subnet of the application-tier. For example, a large subnet with a CIDR block of /16 could be shared by several applications. Denying all inbound non-database traffic from the entire subnet will impact other applications which use this subnet.

    **Setting up the Network ACL of the database subnet to allow inbound database traffic from the subnet of the application-tier** is incorrect because although this solution can work, the subnet of the application-tier could be shared by another tier or another set of EC2 instances other than the application-tier. This means that you would inadvertently be granting database access to unauthorized servers hosted in the same subnet other than the application-tier.

VPC Endpoint is used to allow you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink, but not to the other VPC itself.

**A multinational manufacturing company has multiple accounts in AWS to separate their various departments such as finance, human resources, engineering and many others. There is a requirement to ensure that certain access to services and actions are properly controlled to comply with the security policy of the company.As the Solutions Architect, which is the most suitable way to set up the multi-account AWS environment of the company?**

- ​Set up a common IAM policy that can be applied across all AWS accounts.
- ​Use AWS Organizations and Service Control Policies to control services on each account.
- ​Provide access to externally authenticated users via Identity Federation. Set up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider.
- ​Connect all departments by setting up a cross-account access to each of the AWS accounts of the company. Create and attach IAM policies to your resources based on their respective departments to control access.

### **Explanation**

- Answer

    **Using AWS Organizations and Service Control Policies to control services on each account** is the correct answer. Refer to the diagram below:

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-26_01-25-11-8da501431a6200367e0672f1387defa8.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-26_01-25-11-8da501431a6200367e0672f1387defa8.png)

    AWS Organizations offers policy-based management for multiple AWS accounts. With Organizations, you can create groups of accounts, automate account creation, apply and manage policies for those groups. Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It allows you to create Service Control Policies (SCPs) that centrally control AWS service use across multiple AWS accounts.

    **Setting up a common IAM policy that can be applied across all AWS accounts** is incorrect because it is not possible to create a common IAM policy for multiple AWS accounts.

    The option that says: **Connect all departments by setting up a cross-account access to each of the AWS accounts of the company. Create and attach IAM policies to your resources based on their respective departments to control access** is incorrect because although you can set up cross-account access to each department, this entails a lot of configuration compared with using AWS Organizations and Service Control Policies (SCPs). Cross-account access would be a more suitable choice if you only have two accounts to manage, but not for multiple accounts.

    The option that says: **Provide access to externally authenticated users via Identity Federation. Set up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider** is incorrect as this option is focused on the Identity Federation authentication set up for your AWS accounts but not the IAM policy management for multiple AWS accounts. A combination of AWS Organizations and Service Control Policies (SCPs) is a better choice compared to this option.

**A top university has recently launched its online learning portal where the students can take e-learning courses from the comforts of their homes. The portal is on a large On-Demand EC2 instance with a single Amazon Aurora database. How can you improve the availability of your Aurora database to prevent any unnecessary downtime of the online portal?**

- ​Enable Hash Joins to improve the database query performance.
- ​Create Amazon Aurora Replicas.
- ​Use an Asynchronous Key Prefetch in Amazon Aurora to improve the performance of queries that join tables across indexes.
- ​Deploy Aurora to two Auto-Scaling groups of EC2 instances across two Availability Zones with an elastic load balancer which handles load balancing.

### **Explanation**

- Answer

    Amazon Aurora MySQL and Amazon Aurora PostgreSQL support Amazon Aurora Replicas, which share the same underlying volume as the primary instance. Updates made by the primary are visible to all Amazon Aurora Replicas. With Amazon Aurora MySQL, you can also create MySQL Read Replicas based on MySQL's binlog-based replication engine. In MySQL Read Replicas, data from your primary instance is replayed on your replica as transactions. For most use cases, including read scaling and high availability, it is recommended using Amazon Aurora Replicas.

    ![https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2018/06/08/Aurora-Arch.jpg](https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2018/06/08/Aurora-Arch.jpg)

    Read Replicas are primarily used for improving the read performance of the application. The most suitable solution in this scenario is to use Multi-AZ deployments instead but since this option is not available, you can still set up Read Replicas which you can promote as your primary stand-alone DB cluster in the event of an outage.

    Hence, the correct answer here is to **create Amazon Aurora Replicas**.

    **Deploying Aurora to two Auto-Scaling groups of EC2 instances across two Availability Zones with an elastic load balancer which handles load balancing** is incorrect because Aurora is a database engine for RDS and not deployed on a typical EC2 instance.

    **Enabling Hash Joins to improve the database query performance** is incorrect because Hash Joins are mainly used if you need to join a large amount of data by using an equijoin and not for improving availability.

    **Using an Asynchronous Key Prefetch in Amazon Aurora to improve the performance of queries that join tables across indexes** is incorrect because the Asynchronous Key Prefetch is mainly used to improve the performance of queries that join tables across indexes.

**You are working as a Solutions Architect in a global investment bank which requires corporate IT governance and cost oversight of all of their AWS resources across their divisions around the world. Their corporate divisions want to maintain administrative control of the discrete AWS resources they consume and ensure that those resources are separate from other divisions. Which of the following options will support the autonomy of each corporate division while enabling the corporate IT to maintain governance and cost oversight? (Select TWO.)**

- [ ]  ​Enable IAM cross-account access for all corporate IT administrators in each child account.
- [ ]  ​Create separate Availability Zones for each division within the corporate IT AWS account.
- [ ]  ​Use AWS Consolidated Billing by creating AWS Organizations to link the divisions’ accounts to a parent corporate account.
- [ ]  ​Create separate VPCs for each division within the corporate IT AWS account.
- [ ]  ​Use AWS Trusted Advisor

### **Explanation**

- Answer

    In this scenario, **enabling IAM cross-account access for all corporate IT administrators in each child account** and **using AWS Consolidated Billing by creating AWS Organizations to link the divisions’ accounts to a parent corporate account** are the correct choices. The combined use of IAM and Consolidated Billing will support the autonomy of each corporate division while enabling corporate IT to maintain governance and cost oversight.

    You can use an IAM role to delegate access to resources that are in different AWS accounts that you own. You share resources in one account with users in a different account. By setting up cross-account access in this way, you don't need to create individual IAM users in each account. In addition, users don't have to sign out of one account and sign into another in order to access resources that are in different AWS accounts.

    ![https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/images/BillingBitsOfOrganizations.png](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/images/BillingBitsOfOrganizations.png)

    You can use the consolidated billing feature in AWS Organizations to consolidate payment for multiple AWS accounts or multiple AISPL accounts. With consolidated billing, you can see a combined view of AWS charges incurred by all of your accounts. You can also get a cost report for each member account that is associated with your master account. Consolidated billing is offered at no additional charge. AWS and AISPL accounts can't be consolidated together.

    **Using AWS Trusted Advisor** is incorrect. Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. It only provides you alerts on areas where you do not adhere to best practices and tells you how to improve them. It does not assist in maintaining governance over your AWS accounts.

    **Creating separate VPCs for each division within the corporate IT AWS account** is incorrect because creating separate VPCs would not separate the divisions from each other since they will still be operating under the same account and therefore contribute to the same billing each month.

    **Creating separate Availability Zones for each division within the corporate IT AWS account** is incorrect because you do not need to create Availability Zones. They are already provided for you by AWS right from the start, and not all services support multiple AZ deployments. In addition, having separate Availability Zones in your VPC does not meet the requirement of supporting the autonomy of each corporate division.

**You are working as a Solutions Architect for a major accounting firm, and they have a legacy general ledger accounting application that needs to be moved to AWS. However, the legacy application has a dependency on multicast networking. On this scenario, which of the following options should you consider to ensure the legacy application works in AWS?**

- ​Create a virtual overlay network running on the OS level of the instance.
- ​Provision Elastic Network Interfaces between the subnets.
- ​Create all the subnets on another VPC and enable VPC peering.
- ​All of the above.

### **Explanation**

- Answer

    Multicast is a network capability that allows one-to-many distribution of data. With multicasting, one or more sources can transmit network packets to subscribers that typically reside within a multicast group. However, take note that Amazon VPC does not support multicast or broadcast networking.

    You can use an overlay multicast in order to migrate the legacy application. An overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).

    **Creating a virtual overlay network running on the OS level of the instance** is correct because overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).

    **Provisioning Elastic Network Interfaces between the subnets** is incorrect because just providing ENIs between the subnets would not resolve the dependency on multicast.

    **Creating all the subnets on another VPC and enabling VPC peering** is incorrect because VPC peering and multicast are not the same.

    The option that says: **All of the options are correct** is incorrect because the only option that will work in this scenario is creating a virtual overlay network.

**Which of the following options are not the advantages of using Blue/Green Deployment over in-place upgrade strategy? (Select TWO.)**

- [ ]  ​It has the ability to simply roll the incoming traffic back to the currently working environment, in case of system failures, any time during the deployment process.
- [ ]  ​Blue/green deployment is more cost-effective than in-place upgrade. You don't need to launch a new environment with additional AWS resources.
- [ ]  ​Impaired operation or downtime is minimized because impact is limited to the window of time between green environment issue detection and shift of traffic back to the blue environment.
- [ ]  ​You can use Blue/Green Deployment with CodeCommit and CodeBuild to automatically deploy the new version of your application.
- [ ]  ​Blue/green deployments provide a level of isolation between your blue and green application environments, which reduce the deployment risk. The blue environment represents the current application version serving production traffic while the green one is staged running a different or upgrade version of your application.

### **Explanation**

- Answer

    All of the options are advantages of Blue/Green deployments, except for the following:

    **- You can use Blue/Green Deployment with CodeCommit and CodeBuild to automatically deploy the new version of your application.**

    **Blue/green deployment is more cost-effective than in-place upgrade. You don't need to launch a new environment with additional AWS resources.**

    Take note that the Blue/Green deployment sets up a new ***green*** environment which uses entirely new AWS resources. In addition, CodeCommit and CodeBuild are not used for deployment and hence, it does not relate with Blue/Green deployments.

    Traditionally, with in-place upgrades, it was difficult to validate your new application version in a production deployment while also continuing to run your old version of the application. Blue/green deployments provide a level of isolation between your blue and green application environments. It ensures that spinning up a parallel green environment does not affect resources underpinning your blue environment. This isolation reduces your deployment risk.

    After you deploy the green environment, you have the opportunity to validate it. You might do that with test traffic before sending production traffic to the green environment, or by using a very small fraction of production traffic, to better reflect real user traffic. This is called canary analysis or canary testing. If you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime, and limiting the blast radius of impact.

    This ability to simply roll traffic back to the still-operating blue environment is a key benefit of blue/green deployments. You can roll back to the blue environment at any time during the deployment process. Blue/green deployments also fit well with continuous integration and continuous deployment (CI/CD) workflows, in many cases limiting their complexity. Your deployment automation would have to consider fewer dependencies on an existing environment, state, or configuration.

    In AWS, blue/green deployments also provide cost optimization benefits. You’re not tied to the same underlying resources. So if the performance envelope of the application changes from one version to another, you simply launch the new environment with optimized resources, whether that means fewer resources or just different compute resources. You also don’t have to run an overprovisioned architecture for an extended period of time.

**You are the Solutions Architect of a software development company where you are required to connect the on-premises infrastructure to their AWS cloud. Which of the following AWS services can you use to accomplish this? (Select TWO.)**

- [ ]  ​VPC Peering
- [ ]  ​AWS Direct Connect
- [ ]  ​IPsec VPN connection
- [ ]  ​NAT Gateway
- [ ]  ​Amazon Connect

### **Explanation**

- Answer

    You can connect your VPC to remote networks by using a VPN connection which can be Direct Connect, IPsec VPN connection, AWS VPN CloudHub, or a third party software VPN appliance. Hence, **IPsec VPN connection** and **AWS Direct Connect** are the correct answers.

    **Amazon Connect** is incorrect because this is not a VPN connectivity option. It is actually a self-service, cloud-based contact center service in AWS that makes it easy for any business to deliver better customer service at a lower cost. Amazon Connect is based on the same contact center technology used by Amazon customer service associates around the world to power millions of customer conversations.

    **VPC Peering** is incorrect because this is a networking connection between two VPCs only, which enables you to route traffic between them privately. This can't be used to connect your on-premises network to your VPC.

    **NAT Gateway** is incorrect because you only use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances. This is not used to connect to your on-premises network.

**You have created a VPC with a single subnet then you launched an On-Demand EC2 instance in that subnet. You have attached Internet gateway (IGW) to the VPC and verified that the EC2 instance has a public IP. The main route table of the VPC is as shown below:**

![https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-29_09-51-47-e9ccf269ea2ff9fafe4a307c1257507d.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-29_09-51-47-e9ccf269ea2ff9fafe4a307c1257507d.png)

**However, the instance still cannot be reached from the Internet when you tried to connect to it from your computer.** 

**Which of the following should be made to the route table to fix this issue?**

- ​Add the following entry to the route table: 10.0.0.0/27 -> Your Internet Gateway
- ​Add this new entry to the route table: 0.0.0.0/0 -> Your Internet Gateway
- ​Add this new entry to the route table: 0.0.0.0/27 -> Your Internet Gateway
- ​Modify the above route table: 10.0.0.0/27 -> Your Internet Gateway

### **Explanation**

- Answer

    Apparently, the route table does not have an entry for the Internet Gateway. This is why you cannot connect to the EC2 instance. To fix this, you have to add a route with a destination of `0.0.0.0/0` for IPv4 traffic or `::/0` for IPv6 traffic, and then a target of the Internet gateway ID (`igw-xxxxxxxx`).

    This should be the correct route table configuration after adding the new entry.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-29_10-12-42-b725ca3ed0b358d7a00e8b0fd1c1bc51.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-29_10-12-42-b725ca3ed0b358d7a00e8b0fd1c1bc51.png)

**You are working for a startup which develops an AI-based traffic monitoring service. You need to register a new domain called `www.tutorialsdojo-ai.com` and set up other DNS entries for the other components of your system in AWS. Which of the following is not supported by Amazon Route 53?**

- ​SPF (sender policy framework)
- ​SRV (service locator)
- ​PTR (pointer record)
- ​DNSSEC (Domain Name System Security Extensions)

### **Explanation**

- Answer

    Amazon Route 53’s DNS services does not support DNSSEC at this time. However, their domain name registration service supports configuration of signed DNSSEC keys for domains when DNS service is configured at another provider. More information on configuring DNSSEC for your domain name registration can be found [here](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html).

    Amazon Route 53 currently supports the following DNS record types:

    - -A (address record)
    - -AAAA (IPv6 address record)
    - -CNAME (canonical name record)
    - -CAA (certification authority authorization)
    - -MX (mail exchange record)
    - -NAPTR (name authority pointer record)
    - -NS (name server record)
    - -PTR (pointer record)
    - -SOA (start of authority record)
    - -SPF (sender policy framework)
    - -SRV (service locator)
    - -TXT (text record)

**A web application is hosted on a fleet of EC2 instances inside an Auto Scaling Group with a couple of Lambda functions for ad hoc processing. Whenever you release updates to your application every week, there are inconsistencies where some resources are not updated properly. You need a way to group the resources together and deploy the new version of your code consistently among the groups with minimal downtime. Which among these options should you do to satisfy the given requirement with the least effort?**

- ​Create OpsWorks recipes that will automatically launch resources containing the latest version of the code.
- ​Create CloudFormation templates that have the latest configurations and code in them.
- ​Use deployment groups in CodeDeploy to automate code deployments in a consistent manner.
- ​Use CodeCommit to publish your code quickly in a private repository and push them to your resources for fast updates.

### **Explanation**

- Answer

    CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. It allows you to rapidly release new features, update Lambda function versions, avoid downtime during application deployment, and handle the complexity of updating your applications, without many of the risks associated with error-prone manual deployments.

    ![https://udemy-images.s3.amazonaws.com/redactor/raw/2018-11-03_07-56-49-1bd8f8ea00d8fcd3ab56fa3e74c5a70a.png](https://udemy-images.s3.amazonaws.com/redactor/raw/2018-11-03_07-56-49-1bd8f8ea00d8fcd3ab56fa3e74c5a70a.png)

    **Creating CloudFormation templates that have the latest configurations and code in them** is incorrect since it is used for provisioning and managing stacks of AWS resources based on templates you create to model your infrastructure architecture. CloudFormation is recommended if you want a tool for granular control over the provisioning and management of your own infrastructure.

    **Using CodeCommit to publish your code quickly in a private repository and pushing them to your resources for fast updates** is incorrect as you mainly use CodeCommit for managing a source-control service that hosts private Git repositories. You can store anything from code to binaries and work seamlessly with your existing Git-based tools. CodeCommit integrates with CodePipeline and CodeDeploy to streamline your development and release process.

    You could also use OpsWorks to deploy your code, however, **creating OpsWorks recipes that will automatically launch resources containing the latest version of the code** is still incorrect because you don't need to launch new resources containing your new code when you can just update the ones that are already running.

**A large Philippine-based Business Process Outsourcing company is building a two-tier web application in their VPC to serve dynamic transaction-based content. The data tier is leveraging an Online Transactional Processing (OLTP) database but for the web tier, they are still deciding what service they will use.What AWS services should you leverage to build an elastic and scalable web tier?**

- ​Amazon RDS with Multi-AZ and Auto Scaling
- ​Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3
- ​Elastic Load Balancing, Amazon EC2, and Auto Scaling
- ​Amazon EC2, Amazon DynamoDB, and Amazon S3

### **Explanation**

- Answer

    **Amazon RDS** is a suitable database service for online transaction processing (OLTP) applications. However, the question asks for a list of AWS services for the web tier and not the database tier. Also, when it comes to services providing scalability and elasticity for your web tier, you should always consider using Auto Scaling and Elastic Load Balancer.

    ![https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/tutorial_as_elb_architecture.png](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/tutorial_as_elb_architecture.png)

    To build an elastic and a highly-available web tier, you can use Amazon EC2, Auto Scaling, and Elastic Load Balancing. You can deploy your web servers on a fleet of EC2 instances to an Auto Scaling group, which will automatically monitor your applications and automatically adjust capacity to maintain steady, predictable performance at the lowest possible cost. Load balancing is an effective way to increase the availability of a system. Instances that fail can be replaced seamlessly behind the load balancer while other instances continue to operate. Elastic Load Balancing can be used to balance across instances in multiple availability zones of a region.

    The rest of the options are all incorrect since they don't mention all of the required services in building a highly available and scalable web tier, such as EC2, Auto Scaling, and Elastic Load Balancer. Although Amazon RDS with Multi-AZ and DynamoDB are highly scalable databases, the scenario is more focused on building its web tier and not the database tier.

    Hence, the correct answer is ***Elastic Load Balancing, Amazon EC2, and Auto Scaling.***

    The option that says: ***Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3*** is incorrect because you can't host your web tier using Amazon S3 since the application is doing a dynamic transactions. Amazon S3 is only suitable if you plan to have a static website.

    The option that says: ***Amazon RDS with Multi-AZ and Auto Scaling*** is incorrect because the focus of the question is building a scalable web tier. You need a service, like EC2, in which you can run your web tier.

    The option that says: ***Amazon EC2, Amazon DynamoDB, and Amazon S3*** is incorrect because you need Auto Scaling and ELB in order to scale the web tier.

**You are working for a weather station in Asia with a weather monitoring system that needs to be migrated to AWS. Since the monitoring system requires a low network latency and high network throughput, you decided to launch your EC2 instances to a new cluster placement group. The system was working fine for a couple of weeks, however, when you try to add new instances to the placement group that already has running EC2 instances, you receive an 'insufficient capacity error'.How will you fix this issue?**

- ​Create another Placement Group and launch the new instances in the new group.
- ​Verify all running instances are of the same size and type and then try the launch again.
- ​Submit a capacity increase request to AWS as you are initially limited to only 12 instances per Placement Group.
- ​Stop and restart the instances in the Placement Group and then try the launch again.

### **Explanation**

- Answer

    It is recommended that you launch the number of instances that you need in the placement group in a single launch request and that you use the same instance type for all instances in the placement group. If you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error.

    If you stop an instance in a placement group and then start it again, it still runs in the placement group. However, the start fails if there isn't enough capacity for the instance.

    If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has capacity for all the requested instances.

    The option that says: **Stop and restart the instances in the Placement Group and then try the launch again** is correct because you can resolve this issue just by launching again. If the instances are stopped and restarted, AWS may move the instances to a hardware that has capacity for all the requested instances.

    The option that says: **Create another Placement Group and launch the new instances in the new group** is incorrect because to benefit from the enhanced networking, all the instances should be in the same Placement Group. Launching the new ones in a new Placement Group will not work in this case.

    The option that says: **Verify all running instances are of the same size and type and then try the launch again** is incorrect because the capacity error is not related to the instance size.

    The option that says: **Submit a capacity increase request to AWS as you are initially limited to only 12 instances per Placement Group** is incorrect because there is no such limit on the number of instances in a Placement Group.

**You are a Solutions Architect in an intelligence agency that is currently hosting a learning and training portal in AWS. Your manager instructed you to launch a large EC2 instance with an attached EBS Volume and enable Enhanced Networking. What are the valid case scenarios in using Enhanced Networking? (Select TWO.)**

- [ ]  ​When you need a low packet-per-second performance
- [ ]  ​When you need a dedicated connection to your on-premises data center
- [ ]  ​When you need a consistently lower inter-instance latencies
- [ ]  ​When you need high latency networking
- [ ]  ​When you need a higher packet per second (PPS) performance

### **Explanation**

- Answer

    Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. There is no additional charge for using enhanced networking.

    The option that says: **When you need a low packet-per-second performance** is incorrect because you want to increase packet-per-second performance, and not lower it, when you enable enhanced networking.

    The option that says: **When you need high latency networking** is incorrect because higher latencies means slower network, which is the opposite of what you want to happen when you enable enhanced networking.

    The option that says: **When you need a dedicated connection to your on-premises data center** is incorrect because enabling enhanced networking does not provide a dedicated connection to your on-premises data center. Use AWS Direct Connect or enable VPN tunneling instead for this purpose.

**A company would like to store their old yet confidential corporate files that are infrequently accessed. Which is the MOST cost-efficient solution in AWS that should you recommend?**

- ​Amazon Storage Gateway
- ​Amazon EBS
- ​Amazon S3
- ​Amazon Glacier

### **Explanation**

- Answer

    Amazon Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provides comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Amazon Glacier provides query-in-place functionality, allowing you to run powerful analytics directly on your archive data at rest.

**You were hired as an IT Consultant in a startup cryptocurrency company that wants to go global with their international money transfer app. Your project is to make sure that the database of the app is highly available on multiple regions. What are the benefits of adding Multi-AZ deployments in Amazon RDS? (Select TWO.)**

- [ ]  ​Provides SQL optimization.
- [ ]  ​Provides enhanced database durability in the event of a DB instance component failure or an Availability Zone outage.
- [ ]  ​Increased database availability in the case of system upgrades like OS patching or DB Instance scaling.
- [ ]  ​Significantly increases the database performance.
- [ ]  ​Creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ) in a different region.

### **Explanation**

- Answer

    Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.

    In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.

    ![https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png)

    The chief benefits of running your DB instance as a Multi-AZ deployment are enhanced database durability and availability. The increased availability and fault tolerance offered by Multi-AZ deployments make them a natural fit for production environments.

    Hence, the correct answers are the following options:

    **- Increased database availability in the case of system upgrades like OS patching or DB Instance scaling.**

    **- Provides enhanced database durability in the event of a DB instance component failure or an Availability Zone outage.**

    The option that says: **Creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ) in a different region** is almost correct. RDS synchronously replicates the data to a standby instance in a different Availability Zone (AZ) that is in the same region and not in a different one.

    The options that say: **Significantly increases the database performance** and **Provides SQL optimization** are incorrect as it does not affect the performance nor provide SQL optimization.

**[SAA-C02] You are working as the Solutions Architect for a global technology consultancy firm which has an application that uses multiple EC2 instances located in various AWS regions such as US East (Ohio), US West (N. California), and EU (Ireland). Your manager instructed you to set up a latency-based routing to route incoming traffic for www.tutorialsdojo.com to all the EC2 instances across all AWS regions. Which of the following options can satisfy the given requirement?**

- ​Use an Application Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions.
- ​Use Route 53 to distribute the load to the multiple EC2 instances across all AWS Regions.
- ​Use a Network Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions.
- ​Use AWS DataSync to distribute the load to the multiple EC2 instances across all AWS Regions.

### **Explanation**

- Answer

    If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.

    You can create latency records for your resources in multiple AWS Regions by using latency-based routing. In the event that Route 53 receives a DNS query for your domain or subdomain such as tutorialsdojo.com or portal.tutorialsdojo.com, it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency and then selects a latency record for that region. Route 53 responds with the value from the selected record which can be the IP address for a web server or the CNAME of your elastic load balancer.

    Hence, **using Route 53 to distribute the load to the multiple EC2 instances across all AWS Regions** is the correct answer.

    ![https://media.amazonwebservices.com/blog/weighted_then_geo_1.png](https://media.amazonwebservices.com/blog/weighted_then_geo_1.png)

    **Using a Network Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions** and **using an Application Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions** are both incorrect because load balancers distribute traffic only within their respective regions and not to other AWS regions. It is best to use Route 53 instead to balance the incoming load to two or more AWS regions.

    **Using AWS DataSync to distribute the load to the multiple EC2 instances across all AWS Regions** is incorrect because the AWS DataSync service simply provides a fast way to move large amounts of data online between on-premises storage and Amazon S3 or Amazon Elastic File System (Amazon EFS).

**You are instructed by your manager to create a publicly accessible EC2 instance by using an Elastic IP (EIP) address and also to give him a report on how much it will cost to use that EIP.Which of the following statements is correct regarding the pricing of EIP?**

- ​There is no cost if the instance is stopped and it has only one associated EIP.
- ​There is no cost if the instance is running and it has at least two associated EIP.
- ​There is no cost if the instance is running and it has only one associated EIP.
- ​There is no cost if the instance is terminated and it has only one associated EIP.

### **Explanation**

- Answer

    An Elastic IP address doesn’t incur charges as long as the following conditions are true:

    - -The Elastic IP address is associated with an Amazon EC2 instance.
    - -The instance associated with the Elastic IP address is running.
    - -The instance has only one Elastic IP address attached to it.

    If you’ve stopped or terminated an EC2 instance with an associated Elastic IP address and you don’t need that Elastic IP address anymore, consider disassociating or releasing the Elastic IP address .

**A multinational corporate and investment bank is regularly processing steady workloads of accruals, loan interests, and other critical financial calculations every night at 10 PM to 3 AM on their on-premises data center for their corporate clients. Once the process is done, the results are then uploaded to the Oracle General Ledger which means that the processing should not be delayed nor interrupted. The CTO has decided to move their IT infrastructure to AWS to save cost and to improve the scalability of their digital financial services. As the Senior Solutions Architect, how can you implement a cost-effective architecture in AWS for their financial system?**

- ​Use On-Demand EC2 instances which allows you to pay for the instances that you launch and use by the second.
- ​Use Scheduled Reserved Instances, which provide compute capacity that is always available on the specified recurring schedule.
- ​Use Dedicated Hosts which provide a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-VM software licenses to reduce costs.
- ​Use Spot EC2 Instances launched by a persistent Spot request, which can significantly lower your Amazon EC2 costs.

### **Explanation**

- Answer

    Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them.

    ![https://media.amazonwebservices.com/blog/2015/ec2_sched_ri_find_sched_2.png](https://media.amazonwebservices.com/blog/2015/ec2_sched_ri_find_sched_2.png)

    Scheduled Instances are a good choice for workloads that do not run continuously, but do run on a regular schedule. For example, you can use Scheduled Instances for an application that runs during business hours or for batch processing that runs at the end of the week.

    Hence, the correct answer is to **use Scheduled Reserved Instances, which provide compute capacity that is always available on the specified recurring schedule**.

    **Using On-Demand EC2 instances which allows you to pay for the instances that you launch and use by the second** is incorrect because although an On-Demand instance is stable and suitable for processing critical data, it costs more than any other option. Moreover, the critical financial calculations are only done every night from 10 PM to 3 AM only and not 24/7. This means that your compute capacity will not be utilized for a total of 19 hours every single day.

    **Using Spot EC2 Instances launched by a persistent Spot request, which can significantly lower your Amazon EC2 costs** is incorrect because although this is the most cost-effective solution, this type is not suitable for processing critical financial data since a Spot Instance has a risk of being interrupted.

    **Using Dedicated Hosts which provide a physical host that is fully dedicated to running your instances, and bringing your existing per-socket, per-core, or per-VM software licenses to reduce costs** is incorrect because the use of a fully dedicated physical host is not warranted in this scenario. Moreover, this will be underutilized since you only run the process for 5 hours (from 10 PM to 3 AM only), wasting 19 hours of compute capacity every single day.

**[SAA-C02] You are working for a computer animation film studio that has a web application running on an Amazon EC2 instance. It uploads 5 GB video objects to an Amazon S3 bucket. Video uploads are taking longer than expected, which impacts the performance of your application.Which method will help improve the performance of your application?**

- ​Leverage on Amazon CloudFront and use HTTP POST method to reduce latency.
- ​Use Amazon S3 Multipart Upload API.
- ​Use Amazon Elastic Block Store Provisioned IOPS and an Amazon EBS-optimized instance.
- ​Enable Enhanced Networking with the Elastic Network Adapter (ENA) on your EC2 Instances.

### **Explanation**

- Answer

    The main issue is the slow upload time of the video objects to Amazon S3. To address this issue, you can use Multipart upload in S3 to improve the throughput. It allows you to upload parts of your object in parallel thus, decreasing the time it takes to upload big objects. Each part is a contiguous portion of the object's data.

    You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.

    ![https://media.amazonwebservices.com/blog/s3_multipart_upload.png](https://media.amazonwebservices.com/blog/s3_multipart_upload.png)

    Using multipart upload provides the following advantages:

    **Improved throughput** - You can upload parts in parallel to improve throughput.

    **Quick recovery from any network issues** - Smaller part size minimizes the impact of restarting a failed upload due to a network error.

    **Pause and resume object uploads** - You can upload object parts over time. Once you initiate a multipart upload, there is no expiry; you must explicitly complete or abort the multipart upload.

    **Begin an upload before you know the final object size** - You can upload an object as you are creating it.

    **Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on your EC2 Instances** is incorrect because even though this will improve network performance, the issue will still persist since the problem lies in the upload time of the object to Amazon S3. You should use the Multipart upload feature instead.

    **Leveraging on Amazon CloudFront and using HTTP POST method to reduce latency** is incorrect because CloudFront is a CDN service and is not used to expedite the upload process of objects to Amazon S3. Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

    **Using Amazon Elastic Block Store Provisioned IOPS and an Amazon EBS-optimized instance** is incorrect because although the use of Amazon Elastic Block Store Provisioned IOPS will speed up the I/O performance of the EC2 instance, the root cause is still not resolved since the primary problem here is the slow video upload to Amazon S3. There is no network contention in the EC2 instance.

**You are a Solutions Architect of a tech company. You are having an issue whenever you try to connect to your newly created EC2 instance using a Remote Desktop connection from your computer. Upon checking, you have verified that the instance has a public IP and the Internet gateway and route tables are in place.What else should you do for you to resolve this issue?**

- ​You should restart the EC2 instance since there might be some issue with the instance
- ​You should create a new instance since there might be some issue with the instance
- ​You should adjust the security group to allow traffic from port 22
- ​You should adjust the security group to allow traffic from port 3389

### **Explanation**

- Answer

    Since you are using a Remote Desktop connection to access your EC2 instance, you have to ensure that the Remote Desktop Protocol is allowed in the security group. By default, the server listens on TCP port 3389 and UDP port 3389.

    The option that says: **You should adjust the security group to allow traffic from port 22** is incorrect as the port 22 is used for SSH connections and not for RDP.

    The options that say: **You should restart the EC2 instance since there might be some issue with the instance** and **You should create a new instance since there might be some issue with the instance** are incorrect as the EC2 instance is newly created and hence, unlikely to cause the issue. You have to check the security group first if it allows the Remote Desktop Protocol (3389) before investigating if there is indeed an issue on the specific instance.

**An application is hosted in an Auto Scaling group of EC2 instances and a Microsoft SQL Server on Amazon RDS. There is a requirement that all in-flight data between your web servers and RDS should be secured.Which of the following options is the MOST suitable solution that you should implement? (Select TWO.)**

- [ ]  ​Force all connections to your DB instance to use SSL by setting the `rds.force_ssl` parameter to true. Once done, reboot your DB instance. Configure the security group of your RDS to only allow traffic from port 443.
- [ ]  ​Configure the security groups of your EC2 instances and RDS to only allow traffic to and from port 443.
- [ ]  ​Enable the IAM DB authentication in RDS using the AWS Management Console.
- [ ]  ​Specify the TDE option in an RDS option group that is associated with that DB instance to enable transparent data encryption (TDE).
- [ ]  ​Download the Amazon RDS Root CA certificate. Import the certificate to your servers and configure your application to use SSL to encrypt the connection to RDS.

### **Explanation**

- Answer

    You can use Secure Sockets Layer (SSL) to encrypt connections between your client applications and your Amazon RDS DB instances running Microsoft SQL Server. SSL support is available in all AWS regions for all supported SQL Server editions.

    When you create a SQL Server DB instance, Amazon RDS creates an SSL certificate for it. The SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard against spoofing attacks.

    There are 2 ways to use SSL to connect to your SQL Server DB instance:

    - Force SSL for all connections — this happens transparently to the client, and the client doesn't have to do any work to use SSL.

    - Encrypt specific connections — this sets up an SSL connection from a specific client computer, and you must do work on the client to encrypt connections.

    ![https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/rds_sql_ssl_cert.png](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/rds_sql_ssl_cert.png)

    You can force all connections to your DB instance to use SSL, or you can encrypt connections from specific client computers only. To use SSL from a specific client, you must obtain certificates for the client computer, import certificates on the client computer, and then encrypt the connections from the client computer.

    If you want to force SSL, use the `rds.force_ssl` parameter. By default, the `rds.force_ssl` parameter is set to `false`. Set the `rds.force_ssl` parameter to `true` to force connections to use SSL. The `rds.force_ssl` parameter is static, so after you change the value, you must reboot your DB instance for the change to take effect.

    Hence, the correct answers for this scenario are the options that say:

    **- Force all connections to your DB instance to use SSL by setting the `rds.force_ssl` parameter to true. Once done, reboot your DB instance. Configure the security group of your RDS to only allow traffic from port 443.**

    **- Download the Amazon RDS Root CA certificate. Import the certificate to your servers and configure your application to use SSL to encrypt the connection to RDS.**

    **Specifying the TDE option in an RDS option group that is associated with that DB instance to enable transparent data encryption (TDE)** is incorrect because transparent data encryption (TDE) is primarily used to encrypt stored data on your DB instances running Microsoft SQL Server, and not the data that are in-transit.

    **Enabling the IAM DB authentication in RDS using the AWS Management Console** is incorrect because IAM database authentication is only supported in MySQL and PostgreSQL database engines. With IAM database authentication, you don't need to use a password when you connect to a DB instance but instead, you use an authentication token.

    **Configuring the security groups of your EC2 instances and RDS to only allow traffic to and from port 443** is incorrect because it is not enough to do this. You need to either force all connections to your DB instance to use SSL, or you can encrypt connections from specific client computers, just as mentioned above.

**You are working for a global news network where you have set up a CloudFront distribution for your web application. However, you noticed that your application's origin server is being hit for each request instead of the AWS Edge locations, which serve the cached objects. The issue occurs even for the commonly requested objects.What could be a possible cause of this issue?**

- ​The Cache-Control max-age directive is set to zero.
- ​An object is only cached by Cloudfront once a successful request has been made hence, the objects were not requested before, which is why the request is still directed to the origin server.
- ​You did not add an SSL certificate.
- ​The file sizes of the cached objects are too large for CloudFront to handle.

### **Explanation**

- Answer

    In this scenario, the main culprit is that the Cache-Control max-age directive is set to a low value, which is why the request is always directed to your origin server. Hence the correct answer is the option that says: **The Cache-Control max-age directive is set to zero**.

    The option that says: **An object is only cached by CloudFront once a successful request has been made hence, the objects were not requested before, which is why the request is still directed to the origin server** is incorrect because the issue also occurs even for the commonly requested objects. This means that these objects were successfully requested before but due to a zero Cache-Control max-age directive value, it causes this issue in CloudFront.

    The options that say: **The file sizes of the cached objects are too large for CloudFront to handle** and **You did not add an SSL certificate** are incorrect because they are not related to the issue in caching.

    You can control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin. Reducing the duration allows you to serve dynamic content. Increasing the duration means your users get better performance because your objects are more likely to be served directly from the edge cache. A longer duration also reduces the load on your origin.

    Typically, CloudFront serves an object from an edge location until the cache duration that you specified passes — that is, until the object expires. After it expires, the next time the edge location gets a user request for the object, CloudFront forwards the request to the origin server to verify that the cache contains the latest version of the object.

    The `Cache-Control` and `Expires` headers control how long objects stay in the cache. The `Cache-Control max-age` directive lets you specify how long (in seconds) you want an object to remain in the cache before CloudFront gets the object again from the origin server. The minimum expiration time CloudFront supports is 0 seconds for web distributions and 3600 seconds for RTMP distributions.

**Your company has developed a financial analytics web application hosted in a Docker container using MEAN (MongoDB, Express.js, AngularJS, and Node.js) stack. You want to easily port that web application to AWS Cloud which can automatically handle all the tasks such as balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Which of the following services can be used to fulfill this requirement?**

- ​ECS
- ​AWS Elastic Beanstalk
- ​OpsWorks
- ​AWS CodeDeploy

### **Explanation**

- Answer

    Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can choose your own platform, programming language, and any application dependencies (such as package managers or tools), that aren't supported by other platforms. Docker containers are self-contained and include all the configuration information and software your web application requires to run.

    By using Docker with Elastic Beanstalk, you have an infrastructure that automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. You can manage your web application in an environment that supports the range of services that are integrated with Elastic Beanstalk, including but not limited to VPC, RDS, and IAM. Hence, **AWS Elastic Beanstalk** is the correct answer.

    **ECS** is incorrect because although it also provides Service Auto Scaling, Service Load Balancing and Monitoring with CloudWatch, these features are not ***automatically*** enabled by default unlike with Elastic Beanstalk. Take note that the scenario requires a service that will ***automatically** handle all the tasks such as balancing load, auto-scaling, monitoring, and placing your containers across your cluster.* You will have to manually configure these things if you wish to use ECS. With Elastic Beanstalk, you can manage your web application in an environment that supports the range of services easier.

    **OpsWorks** and **AWS CodeDeploy** are incorrect because these are primarily used for application deployment and configuration only, without providing load balancing, auto-scaling, monitoring or ECS cluster management.

**You are working as a Solutions Architect in a well-funded financial startup. The CTO instructed you to launch a cryptocurrency mining server on a Reserved EC2 instance in us-east-1 region's private subnet which is using IPv6. Due to the financial data that the server contains, the system should be secured to avoid any unauthorized access and to meet the regulatory compliance requirements.In this scenario, which VPC feature allows the EC2 instance to communicate to the Internet but prevents inbound traffic?**

- ​NAT instances
- ​Internet Gateway
- ​Egress-only Internet gateway
- ​NAT Gateway

### **Explanation**

- Answer

    An egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances.

    Take note that an egress-only Internet gateway is for use with IPv6 traffic only. To enable outbound-only Internet communication over IPv4, use a NAT gateway instead.

    ![https://docs.aws.amazon.com/vpc/latest/userguide/images/egress-only-igw-diagram.png](https://docs.aws.amazon.com/vpc/latest/userguide/images/egress-only-igw-diagram.png)

    **NAT Gateway** and **NAT instances** are incorrect because **these are only applicable for IPv4 and not IPv6**. Even though these two components can enable the EC2 instance in a private subnet to communicate to the Internet and prevent inbound traffic, it is only limited with instances which are using IPv4 address and not IPv6. The most suitable VPC component to use is egress-only Internet gateway.

    **Internet Gateway** is incorrect because this is primarily used to provide Internet access to your instances in the public subnet of your VPC, and not for private subnets. However, with an Internet gateway, traffic originating from the public Internet will also be able to reach your instances. The scenario is asking you to prevent inbound access, so this is not the correct answer.

**A web application, which is hosted in your on-premises data center and uses a MySQL database, must be migrated to AWS Cloud. You need to ensure that the network traffic to and from your RDS database instance is encrypted using SSL. For improved security, you have to use the profile credentials specific to your EC2 instance to access your database, instead of a password. Which of the following should you do to meet the above requirement?**

- ​Configure your RDS database to enable encryption.
- ​Launch the mysql client using the `--ssl-ca` parameter when connecting to the database.
- ​Launch a new RDS database instance with the Backtrack feature enabled.
- ​Set up an RDS database and enable the IAM DB Authentication.

### **Explanation**

- Answer

    You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.

    An *authentication token* is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.

    IAM database authentication provides the following benefits:

    - Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).

    - You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.

    - For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security

    Hence, **setting up an RDS database and enable the IAM DB Authentication** is the correct answer based on the above reference.

    **Launching a new RDS database instance with the Backtrack feature enabled** is incorrect because the Backtrack feature simply "rewinds" the DB cluster to the time you specify. Backtracking is not a replacement for backing up your DB cluster so that you can restore it to a point in time. However, you can easily undo mistakes using the backtrack feature if you mistakenly perform a destructive action, such as a `DELETE` without a `WHERE` clause.

    **Configuring your RDS database to enable encryption** is incorrect because this encryption feature in RDS is mainly for securing your Amazon RDS DB instances and snapshots at rest. The data that is encrypted at rest includes the underlying storage for DB instances, its automated backups, Read Replicas, and snapshots.

    **Launching the mysql client using the `--ssl-ca` parameter when connecting to the database** is incorrect because even though using the `--ssl-ca` parameter can provide SSL connection to your database, you still need to use IAM database connection to use the profile credentials specific to your EC2 instance to access your database instead of a password.

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer

asdfasdfasdf

- Answer